{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Super_Piano_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asigalov61/SuperPiano/blob/master/Super_Piano_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9opKSK2RSDRg",
        "colab_type": "text"
      },
      "source": [
        "# Super Piano 3: Google Music Transformer\n",
        "## Generating Music with Long-Term structure\n",
        "### Based on 2019 ICLR paper by Cheng-Zhi Anna Huang, Google Brain and Damon Gwinn code/repo https://github.com/gwinndr/MusicTransformer-Pytorch\n",
        "\n",
        "Huge thanks go out to the following people who contributed the code/repos used in this colab. Additional contributors are listed in the code as well.\n",
        "\n",
        "1) Kevin-Yang https://github.com/jason9693/midi-neural-processor\n",
        "\n",
        "2) jinyi12, Zac Koh, Akamight, Zhang https://github.com/COMP6248-Reproducability-Challenge/music-transformer-comp6248\n",
        "\n",
        "Thank you so much for your hard work and for sharing it with the world :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05hD19W0hSCP",
        "colab_type": "text"
      },
      "source": [
        "###Setup Environment and Dependencies. Check GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ror_UJUp7wlO",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Check if GPU (driver) is avaiiable (you do not want to run this on CPU, trust me)\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paYvoZHihtux",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Clone/Install all dependencies\n",
        "!git clone https://github.com/asigalov61/midi-neural-processor\n",
        "!git clone https://github.com/asigalov61/MusicTransformer-Pytorch\n",
        "!pip install pretty-midi\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install pretty-midi\n",
        "!pip install pypianoroll\n",
        "!pip install matplotlib\n",
        "!pip install librosa\n",
        "!apt install fluidsynth #Pip does not work for some reason. Only apt works\n",
        "!pip install midi2audio\n",
        "!pip install mir_eval\n",
        "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 /content/font.sf2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# For plotting\n",
        "import pypianoroll\n",
        "from pypianoroll import Multitrack, Track\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.use('SVG')\n",
        "import mir_eval.display\n",
        "import librosa\n",
        "import librosa.display\n",
        "# For rendering output audio\n",
        "import pretty_midi\n",
        "from midi2audio import FluidSynth\n",
        "from google.colab import output\n",
        "from IPython.display import display, Javascript, HTML, Audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bGqw8o6oxUY",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Download Google Magenta MAESTRO v.2.0.0 Piano MIDI Dataset (~1300 MIDIs)\n",
        "%cd /content/MusicTransformer-Pytorch/dataset/\n",
        "!wget 'https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip'\n",
        "!unzip maestro-v2.0.0-midi.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXiyUuuonMqM",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Prepare directory sctructure and MIDI processor\n",
        "!mv midi-neural-processor midi_processor\n",
        "!mkdir /content/MusicTransformer-Pytorch/dataset/custom_midi_dataset/\n",
        "%cd /content/MusicTransformer-Pytorch/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN-bpkEGxSMY",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Process MAESTRO MIDI DataSet\n",
        "!python3 preprocess_midi.py '/content/MusicTransformer-Pytorch/dataset/maestro-v2.0.0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsWwjtmfxAaf",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Process NON-MAESTRO MIDI DataSet\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "import processor as midi_processor\n",
        "\n",
        "JSON_FILE = \"maestro-v2.0.0.json\"\n",
        "\n",
        "# prep_midi\n",
        "def prep_midi(maestro_root, output_dir):\n",
        "    \"\"\"\n",
        "    ----------\n",
        "    Author: Damon Gwinn\n",
        "    ----------\n",
        "    Pre-processes the maestro dataset, putting processed midi data (train, eval, test) into the\n",
        "    given output folder\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    train_dir = os.path.join(output_dir, \"train\")\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    val_dir = os.path.join(output_dir, \"val\")\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    test_dir = os.path.join(output_dir, \"test\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    maestro_json_file = os.path.join(maestro_root, JSON_FILE)\n",
        "    if(not os.path.isfile(maestro_json_file)):\n",
        "        print(\"ERROR: Could not find file:\", maestro_json_file)\n",
        "        return False\n",
        "\n",
        "    maestro_json = json.load(open(maestro_json_file, \"r\"))\n",
        "    print(\"Found\", len(maestro_json), \"pieces\")\n",
        "    print(\"Preprocessing...\")\n",
        "\n",
        "    total_count = 0\n",
        "    train_count = 0\n",
        "    val_count   = 0\n",
        "    test_count  = 0\n",
        "\n",
        "    for piece in maestro_json:\n",
        "        mid         = os.path.join(maestro_root, piece[\"midi_filename\"])\n",
        "        split_type  = piece[\"split\"]\n",
        "        f_name      = mid.split(\"/\")[-1] + \".pickle\"\n",
        "\n",
        "        if(split_type == \"train\"):\n",
        "            o_file = os.path.join(train_dir, f_name)\n",
        "            train_count += 1\n",
        "        elif(split_type == \"validation\"):\n",
        "            o_file = os.path.join(val_dir, f_name)\n",
        "            val_count += 1\n",
        "        elif(split_type == \"test\"):\n",
        "            o_file = os.path.join(test_dir, f_name)\n",
        "            test_count += 1\n",
        "        else:\n",
        "            print(\"ERROR: Unrecognized split type:\", split_type)\n",
        "            return False\n",
        "\n",
        "        prepped = midi_processor.encode_midi(mid)\n",
        "\n",
        "        o_stream = open(o_file, \"wb\")\n",
        "        pickle.dump(prepped, o_stream)\n",
        "        o_stream.close()\n",
        "\n",
        "        total_count += 1\n",
        "        if(total_count % 50 == 0):\n",
        "            print(total_count, \"/\", len(maestro_json))\n",
        "\n",
        "    print(\"Num Train:\", train_count)\n",
        "    print(\"Num Val:\", val_count)\n",
        "    print(\"Num Test:\", test_count)\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "# parse_args\n",
        "def parse_args():\n",
        "    \"\"\"\n",
        "    ----------\n",
        "    Author: Damon Gwinn\n",
        "    ----------\n",
        "    Parses arguments for preprocess_midi using argparse\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"maestro_root\", type=str, help=\"Root folder for the Maestro dataset\")\n",
        "    parser.add_argument(\"-output_dir\", type=str, default=\"./dataset/e_piano\", help=\"Output folder to put the preprocessed midi into\")\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "# main\n",
        "def main():\n",
        "    \"\"\"\n",
        "    ----------\n",
        "    Author: Damon Gwinn\n",
        "    ----------\n",
        "    Entry point. Preprocesses maestro and saved midi to specified output folder.\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    args            = parse_args()\n",
        "    maestro_root    = args.maestro_root\n",
        "    output_dir      = args.output_dir\n",
        "\n",
        "    print(\"Preprocessing midi files and saving to\", output_dir)\n",
        "    prep_midi(maestro_root, output_dir)\n",
        "    print(\"Done!\")\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwisXl2Iy_Xf",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Activate Tensorboard Graphs/Stats to monitor/evaluate model perfomance during and after training runs\n",
        "# Load the TensorBoard notebook extension\n",
        "%reload_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "%tensorboard --logdir /content/MusicTransformer-Pytorch/rpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbv_sJyLq5om",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Start to Train the Model\n",
        "batch_size = 4 #@param {type:\"slider\", min:0, max:8, step:1}\n",
        "number_of_training_epochs = 200 #@param {type:\"slider\", min:0, max:200, step:1}\n",
        "maximum_considered_MIDI_sequence = 2048 #@param {type:\"slider\", min:0, max:8192, step:128}\n",
        "!python3 train.py -output_dir rpr --rpr -batch_size=$batch_size -epochs=$number_of_training_epochs -max_sequence=$maximum_considered_MIDI_sequence #-n_layers -num_heads -d_model -dim_feedforward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1D-o-E-TnI8",
        "colab_type": "text"
      },
      "source": [
        "###Evaluate the resulted models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQLOmv7wrOos",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Evaluate Best Resulting Accuracy Model (best_acc_weights.pickle)\n",
        "!python3 evaluate.py -model_weights rpr/results/best_acc_weights.pickle --rpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7QftGOfTyx2",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Evaluate Best Resulting Loss Model (best_loss_weights.pickle)\n",
        "!python3 evaluate.py -model_weights rpr/results/best_loss_weights.pickle --rpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py04K5fn2_2P",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Graph the results\n",
        "!python3 graph_results.py -input_dirs rpr/results -model_names rpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czNulONr4tB6",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Generate, Plot, Graph, Save, Download, and Render the resulting output\n",
        "number_of_tokens_to_generate = 512 #@param {type:\"slider\", min:128, max:8192, step:64}\n",
        "priming_sequence_length = 256 #@param {type:\"slider\", min:0, max:2048, step:8}\n",
        "maximum_possible_output_sequence = 2048 #@param {type:\"slider\", min:0, max:4096, step:128}\n",
        "full_model_path_and_file_name = \"\"\n",
        "select_model = \"/content/MusicTransformer-Pytorch/rpr/results/best_acc_weights.pickle\" #@param [\"/content/MusicTransformer-Pytorch/rpr/results/best_acc_weights.pickle\", \"/content/MusicTransformer-Pytorch/rpr/results/best_loss_weights.pickle\"]\n",
        "\n",
        "!python generate.py -output_dir output -model_weights=$select_model --rpr -target_seq_length=$number_of_tokens_to_generate -num_prime=$priming_sequence_length -max_sequence=$maximum_possible_output_sequence #-primer_file './output/seed.mid' #\n",
        "\n",
        "print('Successfully exported the output to output folder. To primer.mid and rand.mid')\n",
        "\n",
        "# set the src and play\n",
        "FluidSynth(\"/content/font.sf2\").midi_to_audio('/content/MusicTransformer-Pytorch/output/rand.mid', '/content/MusicTransformer-Pytorch/output/output.wav')\n",
        "from google.colab import files\n",
        "files.download('/content/MusicTransformer-Pytorch/output/rand.mid')\n",
        "files.download('/content/MusicTransformer-Pytorch/output/primer.mid')\n",
        "Audio('/content/MusicTransformer-Pytorch/output/output.wav')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMJy63r0Yh9x",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Custom MIDI\n",
        "def prep_midi(maestro_root, output_dir):\n",
        "    \"\"\"\n",
        "    ----------\n",
        "    Author: Damon Gwinn\n",
        "    ----------\n",
        "    Pre-processes the maestro dataset, putting processed midi data (train, eval, test) into the\n",
        "    given output folder\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    train_dir = os.path.join(output_dir, \"train\")\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    val_dir = os.path.join(output_dir, \"val\")\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    test_dir = os.path.join(output_dir, \"test\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    maestro_json_file = os.path.join(maestro_root, JSON_FILE)\n",
        "    if(not os.path.isfile(maestro_json_file)):\n",
        "        print(\"ERROR: Could not find file:\", maestro_json_file)\n",
        "        return False\n",
        "\n",
        "    maestro_json = json.load(open(maestro_json_file, \"r\"))\n",
        "    print(\"Found\", len(maestro_json), \"pieces\")\n",
        "    print(\"Preprocessing...\")\n",
        "\n",
        "    total_count = 0\n",
        "    train_count = 0\n",
        "    val_count   = 0\n",
        "    test_count  = 0\n",
        "\n",
        "    for piece in maestro_json:\n",
        "        mid         = os.path.join(maestro_root, piece[\"midi_filename\"])\n",
        "        split_type  = piece[\"split\"]\n",
        "        f_name      = mid.split(\"/\")[-1] + \".pickle\"\n",
        "\n",
        "        if(split_type == \"train\"):\n",
        "            o_file = os.path.join(train_dir, f_name)\n",
        "            train_count += 1\n",
        "        elif(split_type == \"validation\"):\n",
        "            o_file = os.path.join(val_dir, f_name)\n",
        "            val_count += 1\n",
        "        elif(split_type == \"test\"):\n",
        "            o_file = os.path.join(test_dir, f_name)\n",
        "            test_count += 1\n",
        "        else:\n",
        "            print(\"ERROR: Unrecognized split type:\", split_type)\n",
        "            return False\n",
        "\n",
        "        prepped = midi_processor.encode_midi(mid)\n",
        "\n",
        "        o_stream = open(o_file, \"wb\")\n",
        "        pickle.dump(prepped, o_stream)\n",
        "        o_stream.close()\n",
        "\n",
        "        total_count += 1\n",
        "        if(total_count % 50 == 0):\n",
        "            print(total_count, \"/\", len(maestro_json))\n",
        "\n",
        "    print(\"Num Train:\", train_count)\n",
        "    print(\"Num Val:\", val_count)\n",
        "    print(\"Num Test:\", test_count)\n",
        "    return True\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}