{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Super_Piano_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asigalov61/SuperPiano/blob/master/Super_Piano_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9opKSK2RSDRg",
        "colab_type": "text"
      },
      "source": [
        "# Super Piano 3: Google Music Transformer\n",
        "## Generating Music with Long-Term structure\n",
        "### Based on 2019 ICLR paper by Cheng-Zhi Anna Huang, Google Brain\n",
        "\n",
        "Huge thanks go out to the following people who contributed the code/repos used in this colab. Additional contributors are listed in the code as well.\n",
        "\n",
        "1) Naikaru https://github.com/Naikaru/AI_MusicTransformer\n",
        "\n",
        "2) Kevin-Yang https://github.com/jason9693/midi-neural-processor\n",
        "\n",
        "3) jinyi12, Zac Koh, Akamight, Zhang https://github.com/COMP6248-Reproducability-Challenge/music-transformer-comp6248\n",
        "\n",
        "Thank you so much for your hard work and for sharing it with the world :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05hD19W0hSCP",
        "colab_type": "text"
      },
      "source": [
        "###Setup Environment and Dependencies. Check GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ror_UJUp7wlO",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Check if GPU (driver) is avaiiable (you do not want to run this on CPU, trust me)\n",
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paYvoZHihtux",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Clone/Install all dependencies\n",
        "!git clone https://github.com/asigalov61/SuperPiano\n",
        "!git clone https://github.com/asigalov61/midi-neural-processor\n",
        "!pip install pretty-midi\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwhwdGvU5snk",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Import all needed modules. Check GPU again (if there is no GPU, CPU will be enabled but it will be very slow)\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jsD-2R6OHtW",
        "colab_type": "text"
      },
      "source": [
        "## Setup the MIDI Pre-Processor/Encoder and Directories/Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3cHKI6gOGB4",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Model Main Hyperparameters\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "dim_feedforward = 1024\n",
        "dropout = 0.2\n",
        "num_layer = 6\n",
        "batch_size = 8\n",
        "sequence_length = 1024\n",
        "warmup_steps = 4000\n",
        "pad_token = 1   \n",
        "# vocabulary_size = 388 # depends on the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsOdPRHj4iw-",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Import Special MIDI Encoder\n",
        "%cd /content/midi-neural-processor/\n",
        "\n",
        "from processor import encode_midi, decode_midi\n",
        "\n",
        "%cd /content\n",
        "\n",
        "def encode_midi_files(midi_dir_path, save_dir_path, extension):\n",
        "  #create directory for saving files\n",
        "  os.makedirs(save_dir_path, exist_ok=True)\n",
        "  #get all midi files from midi_directory\n",
        "  for file in os.listdir(midi_dir_path):\n",
        "    if file.endswith(tuple(extension)):\n",
        "        print(os.path.join(midi_dir_path, file))\n",
        "        print(file + ' is being processed', flush=True)\n",
        "        try:\n",
        "          encoded_file = encode_midi(midi_dir_path+file)\n",
        "        except KeyboardInterrupt:\n",
        "            print(' Stopped by keyboard')\n",
        "            return\n",
        "        except EOFError:\n",
        "            print('EOF Error')\n",
        "            return\n",
        "        with open(save_dir_path+file+'.encoded', 'wb') as f:\n",
        "            pickle.dump(encoded_file, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3_Fd7tRDOOC",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Create all IO Directories and Paths\n",
        "!mkdir /content/midi_files\n",
        "!mkdir /content/encoded_midi\n",
        "!mkdir /content/training_set\n",
        "!mkdir /content/test_set\n",
        "%cd /content/\n",
        "\n",
        "\n",
        "midi_dir_path = '/content/midi_files/'\n",
        "save_dir_path = '/content/encoded_midi/'\n",
        "train_dir_path = '/content/training_set/'\n",
        "test_dir_path = '/test_set/'\n",
        "extension = ['.mid', '.midi']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZVOtWMqAq1R",
        "colab_type": "text"
      },
      "source": [
        "#Download and Unzip MIDI DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLbgzvBqkel3",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title (The Best Choice/Works best stand-alone) Super Piano 2 Original 2500 MIDIs of Piano Music\n",
        "%cd /content/midi_files/\n",
        "!wget 'https://github.com/asigalov61/SuperPiano/raw/master/Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip'\n",
        "!unzip -j 'Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip'\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PQUBmnwA1jr",
        "colab_type": "text"
      },
      "source": [
        "#Encode all MIDI files from the DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngsTYWHspwyq",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title RUN ONCE or after you added/removed MIDIs\n",
        "encode_midi_files(midi_dir_path, save_dir_path, extension) # only use once"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMJCA75_fu9M",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Split training data and test data in two different folders\n",
        "# for more convenience we split training data and test data in two different folders\n",
        "\n",
        "import shutil\n",
        "\n",
        "def create_dataset(save_dir_path, split_ratio=0.9):\n",
        "    dataset = [file for file in os.listdir(save_dir_path)]\n",
        "    np.random.shuffle(dataset)\n",
        "\n",
        "    train_set = dataset[:int(len(dataset) * split_ratio)]    \n",
        "    test_set = dataset[int(len(dataset) * split_ratio):]\n",
        "    \n",
        "    shutil.rmtree(train_dir_path)\n",
        "    shutil.rmtree(test_dir_path)\n",
        "\n",
        "    os.makedirs(train_dir_path, exist_ok=True)\n",
        "    os.makedirs(test_dir_path, exist_ok=True)\n",
        "\n",
        "    for file in os.listdir(save_dir_path):\n",
        "        if os.stat(save_dir_path+file).st_size != 0:\n",
        "            if file in test_set:\n",
        "                shutil.copyfile(save_dir_path+file, test_dir_path+file)\n",
        "            else:\n",
        "                shutil.copyfile(save_dir_path+file, train_dir_path+file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urSdAaicuJ2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_dataset(save_dir_path, split_ratio=0.9) # only use once"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LHbi7-HWsfmC",
        "colab": {}
      },
      "source": [
        "def load_dataset(train_dir_path, test_dir_path):\n",
        "    #load all encoded file\n",
        "    train_set = [file for file in os.listdir(train_dir_path)]\n",
        "    test_set = [file for file in os.listdir(test_dir_path)]\n",
        "\n",
        "    return train_set, test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7c3CzQEfwUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(dataset, dir_path, sequence_length=1024, batch_size=8):\n",
        "    sequence_length += 1\n",
        "    batch_midi = []\n",
        "    while len(batch_midi) < batch_size:\n",
        "        file = random.choice(dataset)\n",
        "        #if the midi contains more sequence that the sequence length\n",
        "        try:\n",
        "            with open(dir_path+file, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "        except:\n",
        "            print(dir_path+file + \" file not found.\")\n",
        "        if sequence_length <= len(data):\n",
        "            begin_index = random.randrange(0, len(data) - sequence_length)\n",
        "            data = data[begin_index:begin_index + sequence_length]\n",
        "            batch_midi.append(data)\n",
        "    batch_midi = torch.Tensor(batch_midi)\n",
        "    inputs = batch_midi[:, :-1]\n",
        "    labels = batch_midi[:, 1:]\n",
        "    return inputs, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0dZR6UVPNkd",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waBqGI-AP7qo",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Attention Code Implementation\n",
        "class MusicMultiheadAttention(torch.nn.MultiheadAttention):\n",
        "    def __init__(self, embed_dim, nhead, dropout=0.1, bias=True, add_bias_kv=False, \n",
        "                 add_zero_attn=False, kdim=None, vdim=None):\n",
        "        \n",
        "        torch.nn.MultiheadAttention.__init__(self, embed_dim, nhead, dropout=0.1, \n",
        "                                             bias=True, add_bias_kv=False, \n",
        "                                             add_zero_attn=False, kdim=None, vdim=None)\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.weights_q = torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.weights_k = torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.weights_v = torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.weights_o = torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None,\n",
        "                need_weights=True, attn_mask=None):\n",
        "        Q, K, V = self.transform_input(query, key, value)\n",
        "        # Reshaping the matrices \n",
        "        # Each L × D query, key, and value matrix is then split into H L × D \n",
        "        # h_D parts or attention heads, indexed by h, and with dimension D_h = D/H\n",
        "        Q = self.matrix_to_heads(Q)\n",
        "        K = self.matrix_to_heads(K)\n",
        "        V = self.matrix_to_heads(V)\n",
        "        \n",
        "        # learning a separate relative position embedding Er of shape (H, L, Dh)\n",
        "        Er = torch.randn([self.num_heads, query.size(1), self.head_dim], requires_grad=False).to(device)\n",
        "\n",
        "        # we transpose the two last dimensions of Er to realize Q*Er^T \n",
        "        QEr = torch.matmul(Q, torch.transpose(Er,1,2))\n",
        "        # QEr of shape (B, H, L, L)     \n",
        "        # QEr = torch.einsum('bhld,ld->bhll', [Q, Er])\n",
        "\n",
        "        # 1. Pad a dummy column vector of length L before the leftmost column.\n",
        "        QEr = torch.nn.functional.pad(QEr, (1,0), mode=\"constant\", value=0)\n",
        "\n",
        "        # 2. Reshape the matrix to have shape (L+1, L). \n",
        "        QEr = torch.reshape(QEr, [QEr.size(0), QEr.size(1), QEr.size(3), QEr.size(2)])\n",
        "        \n",
        "        # 3. Slice that matrix to retain only the last l rows and all the columns, \n",
        "        # resulting in a (L, L) matrix again, but now absolute-by-absolute indexed, \n",
        "        # which is the S rel that we need.\n",
        "        S_rel = QEr[:,:,1:,:]\n",
        "\n",
        "        z_attention = self.attention(Q, K, V, S_rel, attn_mask)\n",
        "        z_attention = self.weights_o(z_attention)\n",
        "        # Masking can be added and Dropout ?\n",
        "\n",
        "        return z_attention\n",
        "\n",
        "    def attention(self, Q, K, V, S, mask):\n",
        "        # Dh = self.head_dim // self.num_heads\n",
        "        logits = torch.add(torch.matmul(Q, torch.transpose(K, 2, 3)), S) / math.sqrt((self.head_dim // self.num_heads))\n",
        "        # print(\"logits : \", logits.size())\n",
        "        # print(\"mask : \", mask.size())\n",
        "        if mask is not None:\n",
        "        #    mask = mask.unsqueeze(1) #shape of mask must be broadcastable with shape of underlying tensor\n",
        "            logits = logits.masked_fill(mask == 0, -1e9) #masked_fill fills elements of scores with -1e9 where mask == 0\n",
        "        #if mask is not None:\n",
        "        #    logits += (mask.to(torch.int64) * -1e9).to(logits.dtype)        \n",
        "            \n",
        "        activation = F.softmax(logits, -1)\n",
        "        attention = torch.matmul(activation, V)\n",
        "        attention = torch.reshape(attention, (attention.size(0), -1, self.embed_dim))\n",
        "        return attention\n",
        "    \n",
        "    def matrix_to_heads(self, qkv):\n",
        "        '''\n",
        "            Takes a query/key/value (qkv) matrix and reshapes it to  B * H * L * D_h heads \n",
        "            with dimension D_h = D/H\n",
        "        '''\n",
        "        batch_size_q = qkv.size(0)\n",
        "        #qkv = torch.reshape(qkv, (batch_size_q, qkv.size(0), self.num_heads, self.head_dim))\n",
        "        qkv = torch.reshape(qkv, (batch_size_q, self.num_heads, qkv.size(1), self.head_dim))\n",
        "        return qkv\n",
        "\n",
        "    def transform_input(self, query, key, value):\n",
        "        '''\n",
        "            Transforming the input vector, X, of LxD dimension \n",
        "            into \n",
        "                queries: Q = XW^Q \n",
        "                keys:    K = XW^K\n",
        "            and values:  V = XW^V\n",
        "            which are all DxD square matrices.\n",
        "        '''\n",
        "        return self.weights_q(query), self.weights_k(key), self.weights_v(value)\n",
        "    \n",
        "\n",
        "class MusicTransformerEncoderLayer(torch.nn.TransformerEncoderLayer):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048,\n",
        "                 dropout=0.1, activation=\"relu\"):\n",
        "        torch.nn.TransformerEncoderLayer.__init__(self, d_model, nhead)\n",
        "        self.d_model = d_model\n",
        "        # OverRide\n",
        "        self.self_attn = MusicMultiheadAttention(d_model, nhead)\n",
        "\n",
        "class MusicTransformerDecoderLayer(torch.nn.TransformerDecoderLayer):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048,\n",
        "                 dropout=0.1, activation=\"relu\"):\n",
        "        torch.nn.TransformerDecoderLayer.__init__(self, d_model,nhead)\n",
        "        self.d_model = d_model\n",
        "        # OverRide\n",
        "        self.self_attn = MusicMultiheadAttention(d_model, nhead)\n",
        "\n",
        "class MusicTransformerEncoder(torch.nn.TransformerEncoder):\n",
        "    def __init__(self, encoder_layer, vocabulary_size=390, num_encoder_layers=6, normalization=None):\n",
        "        super().__init__(encoder_layer, num_encoder_layers, normalization)\n",
        "        self.d_model = encoder_layer.d_model\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.dropout = torch.nn.Dropout(encoder_layer.dropout.p)\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=self.vocabulary_size, embedding_dim=self.d_model)\n",
        "    \n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
        "        \n",
        "        pos_encoding = DynamicPositionEmbedding(self.d_model, src.size(1))\n",
        "        \n",
        "        src = math.sqrt(self.d_model) * self.embedding(src.to(torch.long).to(device))\n",
        "        src = pos_encoding(src)\n",
        "        src = self.dropout(src)\n",
        "\n",
        "        return super().forward(src, mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "\n",
        "class MusicTransformerDecoder(torch.nn.TransformerDecoder):\n",
        "    def __init__(self, decoder_layer, vocabulary_size=390, num_decoder_layers=6, normalization=None):\n",
        "        super().__init__(decoder_layer, num_decoder_layers, normalization)\n",
        "        self.d_model = decoder_layer.d_model\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.dropout = torch.nn.Dropout(decoder_layer.dropout.p)\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=self.vocabulary_size, embedding_dim=self.d_model)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, \n",
        "                memory_mask=None, tgt_key_padding_mask=None,\n",
        "                memory_key_padding_mask=None):        \n",
        "                \n",
        "        pos_encoding = DynamicPositionEmbedding(self.d_model, tgt.size(1))\n",
        "\n",
        "        tgt = pos_encoding(math.sqrt(self.d_model) * self.embedding(tgt.to(torch.long).to(device)))\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        return super().forward(tgt, memory, tgt_mask=tgt_mask, \n",
        "                memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "\n",
        "class MusicTransformer(torch.nn.modules.Transformer):\n",
        "    def __init__(self, d_model=512, nhead=8, vocabulary_size=388,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6, \n",
        "                 dim_feedforward=2048, dropout=0.1, activation=\"relu\", \n",
        "                 custom_encoder=None, custom_decoder=None):\n",
        "        \n",
        "        super().__init__(d_model=d_model, nhead=nhead, \n",
        "                         num_encoder_layers=num_encoder_layers,\n",
        "                         num_decoder_layers=num_decoder_layers, \n",
        "                         dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, \n",
        "                         custom_encoder=custom_encoder, custom_decoder=custom_decoder)\n",
        "        \n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        ###        \n",
        "        self.fc = torch.nn.Linear(self.d_model, self.vocabulary_size)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
        "                memory_mask=None, src_key_padding_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "\n",
        "        #if src.size(1) != tgt.size(1):\n",
        "        #    raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
        "\n",
        "        #if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
        "        #    raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
        "            \n",
        "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                              memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        output = self.fc(output)        \n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DynamicPositionEmbedding(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, max_seq=1024):\n",
        "        super().__init__()\n",
        "        embed_sinusoid_list = np.array([[\n",
        "            [\n",
        "                math.sin(\n",
        "                    pos * math.exp(-math.log(10000) * i/embedding_dim) *\n",
        "                    math.exp(math.log(10000)/embedding_dim * (i % 2)) + 0.5 * math.pi * (i % 2)\n",
        "                )\n",
        "                for i in range(embedding_dim)\n",
        "            ]\n",
        "            for pos in range(max_seq)\n",
        "        ]])\n",
        "        self.positional_embedding = embed_sinusoid_list\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + torch.from_numpy(self.positional_embedding[:, :x.size(1), :]).to(x.device, dtype=x.dtype)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoder(torch.nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=1024):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                    math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                    math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x * math.sqrt(self.d_model)\n",
        "            seq_len = x.size(1)\n",
        "            self.pe.to(device)\n",
        "            print(\"self.pe : \", self.pe.device.type)\n",
        "            pe = self.pe[:, :seq_len]\n",
        "            print(\"pe : \", pe.device.type)\n",
        "            x = x + pe\n",
        "            return x   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfU5uhNwKx6u",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Dataset Prep\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "def tensorFromSequence(sequence):\n",
        "    \"\"\"\n",
        "    Generate tensors from the sequence in numpy.\n",
        "    \"\"\"\n",
        "    output = torch.tensor(sequence).long()\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def PrepareData(npz_file, split='train', L=1024):\n",
        "    \"\"\"\n",
        "    Function to prepare the data into pairs (input, target).\n",
        "    Adds [PAD], [SOS] and [EOS] tokens into the data,\n",
        "    where [PAD]=1, [SOS]=2, [EOS]=3.\n",
        "    Limits the sequence to length of L.\n",
        "    \"\"\"\n",
        "    print(\"Preparing data for\",split,\"split...\")\n",
        "    # Load in the data\n",
        "    full_data = np.load(npz_file, fix_imports=True, encoding=\"latin1\", allow_pickle=True)\n",
        "    data = full_data[split]\n",
        "\n",
        "    # Extract the vocab from file\n",
        "    vocab = GenerateVocab(npz_file)\n",
        "    # Generate new vocab to map to later\n",
        "    new_vocab = np.arange(len(vocab))\n",
        "\n",
        "    # Initialize the tokens\n",
        "    pad_token = np.array([[1]])\n",
        "\n",
        "    # Repeat for all samples in data\n",
        "    pairs = []\n",
        "    for samples in data:\n",
        "        # Serialise the dataset so that the resulting sequence is\n",
        "        # S_1 A_1 T_1 B_1, S_2 A_2 T_2 B_2, ...\n",
        "\n",
        "        # Generate input\n",
        "        input_seq = samples.flatten()\n",
        "\n",
        "        # Cut off the samples so that it has length of 1024\n",
        "        if(len(input_seq) >= L):\n",
        "            # input_seq = input_seq[:L-1]\n",
        "            input_seq = input_seq[:L]\n",
        "\n",
        "        # Set the NaN values to 0 and reshape accordingly\n",
        "        input_seq = np.nan_to_num(input_seq.reshape(1,input_seq.size))\n",
        "\n",
        "        # Generate target\n",
        "        output_seq = input_seq[:,1:]\n",
        "\n",
        "        # For both sequences, pad to sequence length L\n",
        "        pad_array = pad_token * np.ones((1,L-input_seq.shape[1]))\n",
        "        input_seq = np.append(input_seq, pad_array,axis=1)\n",
        "        pad_array = pad_token * np.ones((1,L-output_seq.shape[1]))\n",
        "        output_seq = np.append(output_seq, pad_array,axis=1)\n",
        "\n",
        "        # Map the pitch value to int values below vocab size\n",
        "        for i, val in enumerate(vocab):\n",
        "            input_seq[input_seq==val] = new_vocab[i]\n",
        "            output_seq[output_seq==val] = new_vocab[i]\n",
        "\n",
        "        # Make it into a pair\n",
        "        pair = [input_seq, output_seq]\n",
        "\n",
        "        # Combine all pairs into one big list of pairs\n",
        "        pairs.append(pair)\n",
        "\n",
        "    print(\"Generated data pairs.\")\n",
        "    return np.array(pairs)\n",
        "\n",
        "def GenerateVocab(npz_file):\n",
        "    \"\"\"\n",
        "    Generate vocabulary for the dataset including the custom tokens.\n",
        "    \"\"\"\n",
        "    full_data = np.load(npz_file, fix_imports=True, encoding=\"latin1\", allow_pickle=True)\n",
        "    train_data = full_data['train']\n",
        "    validation_data = full_data['valid']\n",
        "    test_data = full_data['test']\n",
        "\n",
        "    combined_data = np.concatenate((train_data, validation_data, test_data))\n",
        "\n",
        "    vocab = np.nan\n",
        "    for sequences in combined_data:\n",
        "        vocab = np.append(vocab,np.unique(sequences))\n",
        "\n",
        "    vocab = np.unique(vocab)\n",
        "    vocab = vocab[~np.isnan(vocab)]\n",
        "    vocab = np.append([0,1],vocab)\n",
        "    return vocab \n",
        " \n",
        "\n",
        "def batched_learning(train,batch_size):\n",
        "    for i in range(0, len(train), batch_size):\n",
        "        train1 = train[i:i + batch_size]\n",
        "        yield train1[:,0],train1[:,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbnfRnF2P0d3",
        "colab_type": "text"
      },
      "source": [
        "##3 - Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtKgQYKuP4-V",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Create and Init the Model\n",
        " \n",
        "# vocabulary_size depends on the midi encoding\n",
        "# ~> 388(+2) for encoded_midi / epiano compt \n",
        "# ~> 46(+2) for encoded_midi / epiano compt                 \n",
        "\n",
        "# DEFINING THE MODEL\n",
        "\n",
        "vocabulary_size = 390\n",
        "\n",
        "normalization = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "custom_encoder_layer = MusicTransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
        "                                               dim_feedforward=dim_feedforward, \n",
        "                                               dropout=dropout, activation=\"relu\")\n",
        "\n",
        "custom_decoder_layer = MusicTransformerDecoderLayer(d_model=d_model, nhead=nhead, \n",
        "                                               dim_feedforward=dim_feedforward, \n",
        "                                               dropout=dropout, activation=\"relu\")\n",
        "\n",
        "custom_encoder = MusicTransformerEncoder(custom_encoder_layer, vocabulary_size, num_layer, normalization)\n",
        "custom_decoder = MusicTransformerDecoder(custom_decoder_layer, vocabulary_size, num_layer, normalization)\n",
        "\n",
        "model = MusicTransformer(d_model=d_model, nhead=nhead, \n",
        "                         vocabulary_size=vocabulary_size, \n",
        "                         num_encoder_layers=num_layer, \n",
        "                         num_decoder_layers=num_layer, \n",
        "                         dim_feedforward=dim_feedforward, \n",
        "                         dropout=dropout, activation=\"relu\", \n",
        "                         custom_encoder=custom_encoder, \n",
        "                         custom_decoder=custom_decoder)\n",
        "# Give model to the current device (hopefully cuda)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "# Adam optimizer [20] with β 1 = 0.9, β 2 = 0.98 and \u000f = 10 −9\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4)\n",
        "\n",
        "# Define a scheduler to vary the learning rate\n",
        "\n",
        "class Scheduler:\n",
        "    def __init__(self, optimizer, d_model=d_model, warmup_steps=4000):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.step_num = 1\n",
        "        self.l_rate = 0\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def step(self):        \n",
        "        # increment step\n",
        "        self.step_num += 1\n",
        "\n",
        "        # compute new learning rate        \n",
        "        self.l_rate = self.d_model**(-.5) * min(self.step_num**(-.5), self.step_num * self.warmup_steps**(-1.5))\n",
        "\n",
        "        # update optimizer learning rate\n",
        "        for p in optimizer.param_groups:\n",
        "            p['lr'] = self.l_rate\n",
        "\n",
        "        # update the weights in the network\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "# See if it is possible to do it using lr_scheduler.LambdaLR lr_scheduler.StepLR\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)\n",
        "scheduler = Scheduler(optimizer, d_model, warmup_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np2MoiTMJ8KF",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Mask Generation\n",
        "### Mask Generation from https://github.com/COMP6248-Reproducability-Challenge/music-transformer-comp6248/blob/master/MaskGen.py\n",
        "\n",
        "# Filename: MaskGen.py\n",
        "# Date Created: 15-Mar-2019 2:42:12 pm\n",
        "# Description: Functions used to generate masks w.r.t. given inputs.\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def nopeak_mask(size):\n",
        "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0).to(device)\n",
        "    return np_mask\n",
        "\n",
        "\n",
        "def create_masks(src, trg, pad_token):\n",
        "    src_mask = (src != pad_token).unsqueeze(-2).to(device)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != pad_token).unsqueeze(-2).to(device)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size)\n",
        "        trg_mask = trg_mask & np_mask\n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask\n",
        "\n",
        "\n",
        "def count_nonpad_tokens(target):\n",
        "    nonpads = (target != 1).squeeze()\n",
        "    ntokens = torch.sum(nonpads)\n",
        "    return ntokens        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_s9IN7xkia9",
        "colab_type": "text"
      },
      "source": [
        "### Training with data from Midi_Encoded "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOSMK5LN1_TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data = load_dataset(train_dir_path, test_dir_path)\n",
        "\n",
        "ckpt_dir = '/content/checkpoints/'\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_loss = 10.\n",
        "model_name = 'midi_encoded_6-1'\n",
        "ckpt_path = '/content/checkpoints/train-nlayer_'+model_name+'.pt'\n",
        "if os.path.exists(ckpt_path):\n",
        "    ckpt = torch.load(ckpt_path)\n",
        "    try:\n",
        "      model.load_state_dict(ckpt['my_model'])\n",
        "      optimizer.load_state_dict(ckpt['optimizer'])\n",
        "      best_acc = ckpt['best_loss']\n",
        "    except RuntimeError as e:\n",
        "        print('wrong checkpoint')\n",
        "    else:    \n",
        "      print('checkpoint is loaded !')\n",
        "      print('current best loss : %.2f' % best_loss)\n",
        "\n",
        "train_writer = SummaryWriter()\n",
        "test_writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYtP5TJlsMRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Tensorboard Graphs and Stats\n",
        "# Load the TensorBoard notebook extension\n",
        "%reload_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "%tensorboard --logdir /content/runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XYfecvLVYkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "outputId": "f83af17e-82b7-4360-a3da-9cb3431a1a96"
      },
      "source": [
        "#@title Start Training\n",
        "# Training\n",
        "max_epochs = 100\n",
        "n_iter = 0\n",
        "\n",
        "# each 50 iterations we are going to compare the losses\n",
        "total_train_loss = []\n",
        "total_valid_loss = []\n",
        "\n",
        "for e in range(max_epochs):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    for b_train in tqdm.tqdm(range(len(train_data) // batch_size)):\n",
        "        \n",
        "        n_iter += 1\n",
        "        # train phase\n",
        "        # feed data into the network and get outputs.\n",
        "        # feed data into the network and get outputs.\n",
        "        inputs, target = generate_batch(train_data, train_dir_path, sequence_length=sequence_length, batch_size=batch_size)\n",
        "              \n",
        "        # Train on GPU\n",
        "        inputs.to(device)\n",
        "        target.to(device)\n",
        "        ys = target.contiguous().view(-1).to(torch.long).to(device)\n",
        "        \n",
        "        # Create mask for both input and target sequences\n",
        "        input_mask, target_mask = create_masks(torch.reshape(inputs, (batch_size, 1, -1)), torch.reshape(target, (batch_size, 1, -1)), pad_token)        \n",
        "        \n",
        "        # feed data into the network and get outputs.\n",
        "        preds_idx = model(inputs, target, input_mask, target_mask)\n",
        "        \n",
        "        # Flush out gradients computed at the previous step before computing gradients at the current step. \n",
        "        #       Otherwise, gradients would accumulate.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate loss\n",
        "        loss = F.cross_entropy(preds_idx.contiguous().view(preds_idx.size(-1), -1).transpose(0,1), ys, ignore_index = pad_token, size_average = False) / (count_nonpad_tokens(ys))\n",
        "\n",
        "        # accumulates the gradient and backprogate loss.\n",
        "        loss.backward()\n",
        "\n",
        "        # performs a parameter update based on the current gradient\n",
        "        scheduler.step()    \n",
        "        \n",
        "        print('\\n====================================================')\n",
        "        print('Epoch/Batch: {}/{}'.format(e, b_train))\n",
        "        print('Train >>>> Loss: {:6.6}'.format(loss))\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    #print('\\n**************************************************')\n",
        "    #print(\"\\n*** Test *** \")\n",
        "    \n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    valid_loss = []\n",
        "    with torch.no_grad():\n",
        "        for b_test in range(len(test_data) // batch_size):        \n",
        "            inputs, target = generate_batch(test_data, test_dir_path, sequence_length=sequence_length, batch_size=batch_size)\n",
        "                  \n",
        "            # Train on GPU\n",
        "            inputs.to(device)\n",
        "            target.to(device)\n",
        "            ys = target.contiguous().view(-1).to(torch.long).to(device)\n",
        "            \n",
        "            # Create mask for both input and target sequences\n",
        "            input_mask, target_mask = create_masks(torch.reshape(inputs, (batch_size, 1,-1)), torch.reshape(target, (batch_size, 1, -1)), pad_token)        \n",
        "\n",
        "            # Feed Forward\n",
        "            preds_validate = model(inputs, target, input_mask, target_mask)\n",
        "            loss = F.cross_entropy(preds_validate.contiguous().view(preds_validate.size(-1), -1).transpose(0,1), ys, \\\n",
        "                                    ignore_index = pad_token, size_average = False) / (count_nonpad_tokens(ys))\n",
        "            valid_loss.append(loss.item())\n",
        "\n",
        "    avg_train_loss = np.mean(train_loss)\n",
        "    avg_valid_loss = np.mean(valid_loss)\n",
        "\n",
        "    total_train_loss.append(avg_train_loss)\n",
        "    total_valid_loss.append(avg_valid_loss)\n",
        "\n",
        "    print(\"[Average Train Loss]: {:6.6}\".format(avg_train_loss))\n",
        "    print(\"[Average Testing Loss]: {:6.6}\".format(avg_valid_loss))\n",
        "\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if avg_valid_loss < best_loss:\n",
        "        best_loss = avg_valid_loss\n",
        "        # Note: optimizer also has states ! don't forget to save them as well.\n",
        "        ckpt = {'my_model':model.state_dict(),\n",
        "                'optimizer':optimizer.state_dict(),\n",
        "                'best_loss':best_loss}\n",
        "        torch.save(ckpt, ckpt_path)\n",
        "        print('checkpoint is saved !')\n",
        "\n",
        "    train_writer.add_scalar('loss/train', avg_train_loss, global_step=n_iter)\n",
        "    test_writer.add_scalar('loss/valid', avg_valid_loss, global_step=n_iter)\n",
        "    #print('\\n**************************************************')\n",
        "\n",
        "train_writer = SummaryWriter()\n",
        "test_writer = SummaryWriter()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r 59%|█████▊    | 170/290 [05:11<03:39,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/169\n",
            "Train >>>> Loss: 5.87859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 59%|█████▉    | 171/290 [05:13<03:37,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/170\n",
            "Train >>>> Loss: 5.85414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 59%|█████▉    | 172/290 [05:15<03:36,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/171\n",
            "Train >>>> Loss: 5.91029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|█████▉    | 173/290 [05:17<03:33,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/172\n",
            "Train >>>> Loss: 5.86714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 174/290 [05:19<03:32,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/173\n",
            "Train >>>> Loss: 5.89046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 175/290 [05:21<03:30,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/174\n",
            "Train >>>> Loss: 5.84257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 61%|██████    | 176/290 [05:22<03:28,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/175\n",
            "Train >>>> Loss: 5.87305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 61%|██████    | 177/290 [05:24<03:26,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/176\n",
            "Train >>>> Loss: 5.89277\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 61%|██████▏   | 178/290 [05:26<03:24,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/177\n",
            "Train >>>> Loss: 5.87278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 179/290 [05:28<03:22,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/178\n",
            "Train >>>> Loss: 5.8581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 180/290 [05:30<03:20,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/179\n",
            "Train >>>> Loss: 5.88501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 181/290 [05:32<03:18,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/180\n",
            "Train >>>> Loss: 5.86344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 63%|██████▎   | 182/290 [05:33<03:16,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/181\n",
            "Train >>>> Loss: 5.85615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 63%|██████▎   | 183/290 [05:35<03:15,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/182\n",
            "Train >>>> Loss: 5.85881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 63%|██████▎   | 184/290 [05:37<03:13,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/183\n",
            "Train >>>> Loss: 5.86897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 185/290 [05:39<03:11,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/184\n",
            "Train >>>> Loss: 5.87244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 186/290 [05:41<03:09,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/185\n",
            "Train >>>> Loss: 5.85547\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 187/290 [05:42<03:07,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/186\n",
            "Train >>>> Loss: 5.88189\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 65%|██████▍   | 188/290 [05:44<03:05,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/187\n",
            "Train >>>> Loss: 5.85982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 65%|██████▌   | 189/290 [05:46<03:04,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/188\n",
            "Train >>>> Loss: 5.86864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 66%|██████▌   | 190/290 [05:48<03:02,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/189\n",
            "Train >>>> Loss: 5.90305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 66%|██████▌   | 191/290 [05:50<03:00,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/190\n",
            "Train >>>> Loss: 5.88543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 66%|██████▌   | 192/290 [05:52<02:58,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/191\n",
            "Train >>>> Loss: 5.85767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 193/290 [05:53<02:57,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/192\n",
            "Train >>>> Loss: 5.90446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 194/290 [05:55<02:54,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/193\n",
            "Train >>>> Loss:  5.881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 195/290 [05:57<02:52,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/194\n",
            "Train >>>> Loss: 5.88915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 196/290 [05:59<02:51,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/195\n",
            "Train >>>> Loss: 5.90721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 197/290 [06:01<02:49,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/196\n",
            "Train >>>> Loss: 5.86305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 198/290 [06:02<02:47,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/197\n",
            "Train >>>> Loss: 5.89463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 69%|██████▊   | 199/290 [06:04<02:45,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/198\n",
            "Train >>>> Loss: 5.88459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 69%|██████▉   | 200/290 [06:06<02:44,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/199\n",
            "Train >>>> Loss: 5.89994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 69%|██████▉   | 201/290 [06:08<02:43,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/200\n",
            "Train >>>> Loss: 5.86162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|██████▉   | 202/290 [06:10<02:41,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/201\n",
            "Train >>>> Loss: 5.87455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 203/290 [06:12<02:39,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/202\n",
            "Train >>>> Loss: 5.85298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 204/290 [06:13<02:37,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/203\n",
            "Train >>>> Loss: 5.90132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 71%|███████   | 205/290 [06:15<02:35,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/204\n",
            "Train >>>> Loss: 5.84119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 71%|███████   | 206/290 [06:17<02:33,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/205\n",
            "Train >>>> Loss: 5.86502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 71%|███████▏  | 207/290 [06:19<02:31,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/206\n",
            "Train >>>> Loss: 5.87978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 72%|███████▏  | 208/290 [06:21<02:29,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/207\n",
            "Train >>>> Loss: 5.8859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 72%|███████▏  | 209/290 [06:23<02:27,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/208\n",
            "Train >>>> Loss: 5.84231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 72%|███████▏  | 210/290 [06:24<02:26,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/209\n",
            "Train >>>> Loss: 5.83269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 211/290 [06:26<02:24,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/210\n",
            "Train >>>> Loss: 5.88068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 212/290 [06:28<02:22,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/211\n",
            "Train >>>> Loss: 5.84315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 213/290 [06:30<02:20,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/212\n",
            "Train >>>> Loss: 5.87322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 74%|███████▍  | 214/290 [06:32<02:19,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/213\n",
            "Train >>>> Loss: 5.89127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 74%|███████▍  | 215/290 [06:34<02:16,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/214\n",
            "Train >>>> Loss: 5.89715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 74%|███████▍  | 216/290 [06:35<02:15,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/215\n",
            "Train >>>> Loss: 5.87049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 75%|███████▍  | 217/290 [06:37<02:13,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/216\n",
            "Train >>>> Loss: 5.91095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 75%|███████▌  | 218/290 [06:39<02:11,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/217\n",
            "Train >>>> Loss: 5.88499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 219/290 [06:41<02:09,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/218\n",
            "Train >>>> Loss: 5.86966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 220/290 [06:43<02:07,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/219\n",
            "Train >>>> Loss: 5.88307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 221/290 [06:45<02:05,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/220\n",
            "Train >>>> Loss: 5.87204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 77%|███████▋  | 222/290 [06:46<02:04,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/221\n",
            "Train >>>> Loss: 5.8742\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 77%|███████▋  | 223/290 [06:48<02:02,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/222\n",
            "Train >>>> Loss:  5.883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 77%|███████▋  | 224/290 [06:50<02:00,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/223\n",
            "Train >>>> Loss: 5.82975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 225/290 [06:52<01:58,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/224\n",
            "Train >>>> Loss: 5.83616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 226/290 [06:54<01:57,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/225\n",
            "Train >>>> Loss: 5.84642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 227/290 [06:56<01:55,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/226\n",
            "Train >>>> Loss: 5.91602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 79%|███████▊  | 228/290 [06:57<01:53,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/227\n",
            "Train >>>> Loss: 5.89037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 79%|███████▉  | 229/290 [06:59<01:51,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/228\n",
            "Train >>>> Loss: 5.90602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 79%|███████▉  | 230/290 [07:01<01:49,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/229\n",
            "Train >>>> Loss: 5.92208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|███████▉  | 231/290 [07:03<01:47,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/230\n",
            "Train >>>> Loss: 5.86361\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 232/290 [07:05<01:45,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/231\n",
            "Train >>>> Loss: 5.88511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 233/290 [07:06<01:44,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/232\n",
            "Train >>>> Loss: 5.87148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 81%|████████  | 234/290 [07:08<01:42,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/233\n",
            "Train >>>> Loss: 5.88079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 81%|████████  | 235/290 [07:10<01:40,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/234\n",
            "Train >>>> Loss: 5.90962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 81%|████████▏ | 236/290 [07:12<01:38,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/235\n",
            "Train >>>> Loss: 5.87023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 237/290 [07:14<01:36,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/236\n",
            "Train >>>> Loss: 5.89173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 238/290 [07:16<01:35,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/237\n",
            "Train >>>> Loss:  5.903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 239/290 [07:17<01:33,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/238\n",
            "Train >>>> Loss: 5.86212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 240/290 [07:19<01:31,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/239\n",
            "Train >>>> Loss: 5.8647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 241/290 [07:21<01:29,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/240\n",
            "Train >>>> Loss: 5.85361\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 242/290 [07:23<01:27,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/241\n",
            "Train >>>> Loss: 5.86521\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 243/290 [07:25<01:25,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/242\n",
            "Train >>>> Loss: 5.84689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 244/290 [07:27<01:23,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/243\n",
            "Train >>>> Loss: 5.90783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 245/290 [07:28<01:22,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/244\n",
            "Train >>>> Loss: 5.83394\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 85%|████████▍ | 246/290 [07:30<01:20,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/245\n",
            "Train >>>> Loss: 5.84492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 85%|████████▌ | 247/290 [07:32<01:18,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/246\n",
            "Train >>>> Loss: 5.88521\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 248/290 [07:34<01:16,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/247\n",
            "Train >>>> Loss: 5.86782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 249/290 [07:36<01:14,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/248\n",
            "Train >>>> Loss: 5.86979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 250/290 [07:38<01:13,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/249\n",
            "Train >>>> Loss: 5.8721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 251/290 [07:39<01:11,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/250\n",
            "Train >>>> Loss: 5.89378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 252/290 [07:41<01:09,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/251\n",
            "Train >>>> Loss: 5.88254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 253/290 [07:43<01:07,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/252\n",
            "Train >>>> Loss: 5.87999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 254/290 [07:45<01:05,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/253\n",
            "Train >>>> Loss: 5.85106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 255/290 [07:47<01:03,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/254\n",
            "Train >>>> Loss: 5.8827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 256/290 [07:49<01:02,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/255\n",
            "Train >>>> Loss: 5.87418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 89%|████████▊ | 257/290 [07:50<01:00,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/256\n",
            "Train >>>> Loss: 5.84674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 89%|████████▉ | 258/290 [07:52<00:58,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/257\n",
            "Train >>>> Loss: 5.86594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 89%|████████▉ | 259/290 [07:54<00:56,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/258\n",
            "Train >>>> Loss: 5.9179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|████████▉ | 260/290 [07:56<00:54,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/259\n",
            "Train >>>> Loss: 5.87041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 261/290 [07:58<00:52,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/260\n",
            "Train >>>> Loss: 5.87535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 262/290 [07:59<00:51,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/261\n",
            "Train >>>> Loss: 5.87399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 263/290 [08:01<00:49,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/262\n",
            "Train >>>> Loss: 5.85861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 264/290 [08:03<00:47,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/263\n",
            "Train >>>> Loss: 5.8535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████▏| 265/290 [08:05<00:45,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/264\n",
            "Train >>>> Loss: 5.84807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 266/290 [08:07<00:43,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/265\n",
            "Train >>>> Loss: 5.91478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 267/290 [08:09<00:42,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/266\n",
            "Train >>>> Loss: 5.89147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 268/290 [08:10<00:40,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/267\n",
            "Train >>>> Loss: 5.86361\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 269/290 [08:12<00:38,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/268\n",
            "Train >>>> Loss: 5.87768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 270/290 [08:14<00:36,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/269\n",
            "Train >>>> Loss: 5.86707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 271/290 [08:16<00:35,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/270\n",
            "Train >>>> Loss: 5.8573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 94%|█████████▍| 272/290 [08:18<00:33,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/271\n",
            "Train >>>> Loss: 5.90703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 94%|█████████▍| 273/290 [08:20<00:31,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/272\n",
            "Train >>>> Loss: 5.86831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 94%|█████████▍| 274/290 [08:21<00:29,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/273\n",
            "Train >>>> Loss: 5.86054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 95%|█████████▍| 275/290 [08:23<00:27,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/274\n",
            "Train >>>> Loss: 5.87766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 95%|█████████▌| 276/290 [08:25<00:25,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/275\n",
            "Train >>>> Loss: 5.87726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 277/290 [08:27<00:23,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/276\n",
            "Train >>>> Loss: 5.85965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 278/290 [08:29<00:22,  1.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/277\n",
            "Train >>>> Loss: 5.83895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 279/290 [08:31<00:20,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/278\n",
            "Train >>>> Loss: 5.88173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 97%|█████████▋| 280/290 [08:33<00:18,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/279\n",
            "Train >>>> Loss: 5.89214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 97%|█████████▋| 281/290 [08:34<00:16,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/280\n",
            "Train >>>> Loss: 5.88817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 97%|█████████▋| 282/290 [08:36<00:14,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/281\n",
            "Train >>>> Loss: 5.87259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 98%|█████████▊| 283/290 [08:38<00:12,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/282\n",
            "Train >>>> Loss: 5.89176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 98%|█████████▊| 284/290 [08:40<00:10,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/283\n",
            "Train >>>> Loss: 5.87077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 98%|█████████▊| 285/290 [08:42<00:09,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/284\n",
            "Train >>>> Loss: 5.89316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 99%|█████████▊| 286/290 [08:43<00:07,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/285\n",
            "Train >>>> Loss: 5.90742\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 99%|█████████▉| 287/290 [08:45<00:05,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/286\n",
            "Train >>>> Loss: 5.8981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 99%|█████████▉| 288/290 [08:47<00:03,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/287\n",
            "Train >>>> Loss: 5.92574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 289/290 [08:49<00:01,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/288\n",
            "Train >>>> Loss: 5.85131\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 290/290 [08:51<00:00,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/289\n",
            "Train >>>> Loss: 5.91338\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/290 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Average Train Loss]: 5.87723\n",
            "[Average Testing Loss]: 5.88378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 1/290 [00:01<08:59,  1.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/0\n",
            "Train >>>> Loss: 5.90702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  1%|          | 2/290 [00:03<08:55,  1.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/1\n",
            "Train >>>> Loss: 5.86553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  1%|          | 3/290 [00:05<08:51,  1.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/2\n",
            "Train >>>> Loss: 5.90569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  1%|▏         | 4/290 [00:07<08:47,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/3\n",
            "Train >>>> Loss: 5.91822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 5/290 [00:09<08:43,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/4\n",
            "Train >>>> Loss: 5.90035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 6/290 [00:11<08:40,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/5\n",
            "Train >>>> Loss: 5.87782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 7/290 [00:12<08:39,  1.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/6\n",
            "Train >>>> Loss: 5.85824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 8/290 [00:14<08:36,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/7\n",
            "Train >>>> Loss: 5.9074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 9/290 [00:16<08:34,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/8\n",
            "Train >>>> Loss: 5.89129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 10/290 [00:18<08:31,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/9\n",
            "Train >>>> Loss: 5.85539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 11/290 [00:20<08:31,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/10\n",
            "Train >>>> Loss: 5.87485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 12/290 [00:21<08:28,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/11\n",
            "Train >>>> Loss: 5.85324\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 13/290 [00:23<08:25,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/12\n",
            "Train >>>> Loss: 5.84745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  5%|▍         | 14/290 [00:25<08:23,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/13\n",
            "Train >>>> Loss: 5.86305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 15/290 [00:27<08:20,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/14\n",
            "Train >>>> Loss: 5.8566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 16/290 [00:29<08:18,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/15\n",
            "Train >>>> Loss: 5.91288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 17/290 [00:31<08:17,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/16\n",
            "Train >>>> Loss: 5.87495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 18/290 [00:32<08:17,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/17\n",
            "Train >>>> Loss: 5.90463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 19/290 [00:34<08:16,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/18\n",
            "Train >>>> Loss: 5.87565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 20/290 [00:36<08:14,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/19\n",
            "Train >>>> Loss: 5.90156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 21/290 [00:38<08:11,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/20\n",
            "Train >>>> Loss: 5.87591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 22/290 [00:40<08:10,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/21\n",
            "Train >>>> Loss: 5.89949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 23/290 [00:42<08:07,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/22\n",
            "Train >>>> Loss: 5.89283\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 24/290 [00:43<08:05,  1.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/23\n",
            "Train >>>> Loss: 5.84822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▊         | 25/290 [00:45<08:04,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/24\n",
            "Train >>>> Loss: 5.88076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 26/290 [00:47<08:02,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/25\n",
            "Train >>>> Loss: 5.82566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 27/290 [00:49<08:01,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/26\n",
            "Train >>>> Loss: 5.87919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|▉         | 28/290 [00:51<08:00,  1.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/27\n",
            "Train >>>> Loss: 5.90984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-01659f9ece61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Flush out gradients computed at the previous step before computing gradients at the current step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#       Otherwise, gradients would accumulate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m                           \u001b[0;34m\"non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                           \"github.com/pytorch/pytorch/pull/30531 for more informations.\", stacklevel=2)\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTqvzYhl05eK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(total_train_loss, label='train loss')\n",
        "plt.plot(total_valid_loss, label='validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "img_path = os.path.join('/gdrive/My Drive/my_data/library/checkpoints/', \"encoded_midi_6-1.png\")\n",
        "plt.savefig(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR1FaRoBkvSl",
        "colab_type": "text"
      },
      "source": [
        "### Training with data from JS Bach Chorales "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9o7LM7gkfZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_data = '/gdrive/My Drive/my_data/library/Jsb16thSeparated.npz'\n",
        "\n",
        "# Generate the vocabulary from the data\n",
        "vocab = GenerateVocab(src_data)\n",
        "vocabulary_size = len(vocab)\n",
        "pad_token = 1\n",
        "\n",
        "# since we change de vocab len the model should be create after this\n",
        "# we just show this as an exemple of training of JS Bach Chorales dataset \n",
        "# thanks to propressing functions from another repository.\n",
        "\n",
        "# Setup the dataset for training split and validation split\n",
        "train_data = PrepareData(src_data ,'train', int(sequence_length))\n",
        "valid_data = PrepareData(src_data ,'valid', int(sequence_length))\n",
        "\n",
        "ckpt_dir = os.path.join(gdrive_root, '/my_data/library/checkpoints/')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_loss = 10.\n",
        "model_name = 'midi_encoded_6-1'\n",
        "ckpt_path = '/gdrive/My Drive/my_data/library/checkpoints/train-nlayer_'+model_name+'.pt'\n",
        "if os.path.exists(ckpt_path):\n",
        "    ckpt = torch.load(ckpt_path)\n",
        "    try:\n",
        "      model.load_state_dict(ckpt['my_model'])\n",
        "      optimizer.load_state_dict(ckpt['optimizer'])\n",
        "      best_acc = ckpt['best_loss']\n",
        "    except RuntimeError as e:\n",
        "        print('wrong checkpoint')\n",
        "    else:    \n",
        "      print('checkpoint is loaded !')\n",
        "      print('current best loss : %.2f' % best_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjr9R_wDjwHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "max_epochs = 100\n",
        "n_iter = 0\n",
        "\n",
        "# each 50 iterations we are going to compare the losses\n",
        "total_train_loss = []\n",
        "total_valid_loss = []\n",
        "\n",
        "for e in range(max_epochs):\n",
        "    model.train()\n",
        "    random.shuffle(train_data)\n",
        "    train_loss = []\n",
        "    for b_train, batch in enumerate(batched_learning(train_data, batch_size=batch_size)):        \n",
        "        \n",
        "        n_iter += 1\n",
        "        # train phase\n",
        "        # feed data into the network and get outputs.\n",
        "        inputs, target = batch\n",
        "        \n",
        "        #print(inputs.shape, target.shape)\n",
        "\n",
        "        # Train on GPU\n",
        "        inputs = (tensorFromSequence(inputs)).to(device)\n",
        "        target = (tensorFromSequence(target)).to(device)\n",
        "\n",
        "        # Create mask for both input and target sequences\n",
        "        input_mask, target_mask = create_masks(inputs, target, pad_token)\n",
        "\n",
        "        inputs = inputs[:,0,:]\n",
        "        target = target[:,0,:] \n",
        "\n",
        "        ys = target.contiguous().view(-1)              \n",
        "\n",
        "        # ys = labels.contiguous().view(-1).to(torch.long).to(device)\n",
        "        \n",
        "        # feed data into the network and get outputs.\n",
        "        preds_idx = model(inputs, target, input_mask, target_mask)\n",
        "        \n",
        "        # Flush out gradients computed at the previous step before computing gradients at the current step. \n",
        "        #       Otherwise, gradients would accumulate.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate loss\n",
        "        loss = F.cross_entropy(preds_idx.contiguous().view(preds_idx.size(-1), -1).transpose(0,1), ys, ignore_index = pad_token, size_average = False) / (count_nonpad_tokens(ys))\n",
        "\n",
        "        # accumulates the gradient and backprogate loss.\n",
        "        loss.backward()\n",
        "\n",
        "        # performs a parameter update based on the current gradient\n",
        "        scheduler.step()    \n",
        "        \n",
        "        print('\\n====================================================')\n",
        "        print('Epoch/Batch: {}/{}'.format(e, b_train))\n",
        "        print('Train >>>> Loss: {:6.6}'.format(loss))\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    print('\\n**************************************************')\n",
        "    print(\"\\n*** Test *** \")\n",
        "    \n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    valid_loss = []\n",
        "    with torch.no_grad():\n",
        "        pair = valid_data\n",
        "        inputs = tensorFromSequence(pair[0]).to(device)\n",
        "        target = tensorFromSequence(pair[1]).to(device)\n",
        "        \n",
        "        # Create mask for both input and target sequences\n",
        "        input_mask, target_mask = create_masks(inputs, target, pad_token)\n",
        "\n",
        "        inputs = inputs[:,0,:]\n",
        "        target = target[:,0,:] \n",
        "        ys = target.contiguous().view(-1)\n",
        "\n",
        "        preds_validate = model(inputs, target, input_mask, target_mask)\n",
        "        loss = F.cross_entropy(preds_validate.contiguous().view(preds_validate.size(-1), -1).transpose(0,1), ys, \\\n",
        "                                ignore_index = pad_token, size_average = False) / (count_nonpad_tokens(ys))\n",
        "        valid_loss.append(loss.item())\n",
        "\n",
        "    avg_train_loss = np.mean(train_loss)\n",
        "    avg_valid_loss = np.mean(valid_loss)\n",
        "\n",
        "    total_train_loss.append(avg_train_loss)\n",
        "    total_valid_loss.append(avg_valid_loss)\n",
        "\n",
        "    print(\"[Average Train Loss]: {:6.6}\".format(avg_train_loss))\n",
        "    print(\"[Average Testing Loss]: {:6.6}\".format(avg_valid_loss))\n",
        "\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if avg_valid_loss < best_loss:\n",
        "        best_loss = avg_valid_loss\n",
        "        # Note: optimizer also has states ! don't forget to save them as well.\n",
        "        ckpt = {'my_model':model.state_dict(),\n",
        "                'optimizer':optimizer.state_dict(),\n",
        "                'best_loss':best_loss}\n",
        "        torch.save(ckpt, ckpt_path)\n",
        "        print('checkpoint is saved !')\n",
        "\n",
        "    train_writer.add_scalar('loss/train', avg_train_loss, global_step=n_iter)\n",
        "    test_writer.add_scalar('loss/valid', avg_valid_loss, global_step=n_iter)\n",
        "    print('\\n**************************************************')\n",
        "    # torch.cuda.empty_cache()\n",
        "    # torch.save(model.state_dict(), '/gdrive/My Drive/my_data/library/checkpoints/train-{}.pt'.format(e))\n",
        "    ckpt = {'my_model':model.state_dict(),\n",
        "            'optimizer':optimizer.state_dict(),\n",
        "            'best_loss':best_loss}\n",
        "    torch.save(model.state_dict(), ckpt_backup_path)\n",
        "    torch.save(ckpt, ckpt_fullbackup_path)\n",
        "\n",
        "train_writer = SummaryWriter()\n",
        "test_writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1PVt1y82Gzy",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK1Z-ijf2Gd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "dbe2418a-54af-4836-885c-91852ad79d4c"
      },
      "source": [
        "inputs = torch.randint(0, 1000, (1,10)).to(device)\n",
        "gen_length = 30\n",
        "generated_midi = torch.Tensor()\n",
        "for seq in range(gen_length):\n",
        "    if seq % 20 == 0: \n",
        "        print(seq)\n",
        "    #print(inputs)\n",
        "    logits = F.softmax(model(inputs[:, :-1], inputs[:, 1:]), -1).to(device)\n",
        "\n",
        "    logits = logits[0, :, :]\n",
        "    \n",
        "    one_hot = torch.distributions.OneHotCategorical(probs=logits[:,-1])\n",
        "    res = one_hot.sample().argmax(-1).unsqueeze(-1).to(device)\n",
        "\n",
        "    #res = torch.transpose(res, 0, 1)\n",
        "\n",
        "    inputs = torch.cat((inputs[0], res), dim=-1)\n",
        "    inputs = torch.reshape(inputs, (1, -1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b4ce3b1567a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8d9b4e64bbaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m#    raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[1;32m    175\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8d9b4e64bbaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8d9b4e64bbaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qjnWiWn2a4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "83ffd949-6142-41fd-f3ba-8532c324831c"
      },
      "source": [
        "from matplotlib import pyplot as pt\n",
        "\n",
        "x = range(0, len(inputs[0]))\n",
        "y = [ints.item() for ints in inputs[0]]\n",
        "\n",
        "p = pt.scatter(x, y, label='generated pitches')\n",
        "\n",
        "pt.xlabel('index')\n",
        "pt.ylabel('midi value (pitch)')\n",
        "pt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b21b3a7a8362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mints\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'generated pitches'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-b21b3a7a8362>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mints\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'generated pitches'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl7nrHJK4tlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "midi_boy = decode_midi(inputs[0])\n",
        "torch.save(midi_boy, gdrive_root + '/my_data/library/test.midi')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}