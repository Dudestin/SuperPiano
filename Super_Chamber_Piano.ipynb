{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "[Ver 2.0] Super Chamber Piano.ipynb",
      "provenance": [],
      "private_outputs": true,
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dudestin/SuperPiano/blob/master/Super_Chamber_Piano.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIqS2usIWHeI"
      },
      "source": [
        "# Super Chamber Piano (Ver 2.0): Notewise Music NN\n",
        "\n",
        "***\n",
        "\n",
        "## A Mini Musical Neural Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5SX5WuiWThz"
      },
      "source": [
        "***\n",
        "\n",
        "All thanks and credit for this beautiful colab go out to edtky of GitHub on whose repo and code it is based: https://github.com/edtky/mini-musical-neural-net\n",
        "\n",
        "***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML_w70Rcod6f"
      },
      "source": [
        "# Model Specs and Default Parameters\n",
        "\n",
        "## 2 Layers LSTM\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "1. sequence_len / n_word = 512 (previous: 128)\n",
        "2. batch_size / n_seq = 128\n",
        "3. hidden_dim = 512\n",
        "4. top_k words = 3 (previous: 5)\n",
        "5. predict seq_len = 512 (previous: 1024)\n",
        "6. epoch = 300\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isGj8yhe0DvT"
      },
      "source": [
        "# System setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVrjMED_5sCO"
      },
      "source": [
        "#@title Install dependencies\n",
        "!pip install git+https://github.com/kroger/pyknon.git\n",
        "!pip install pretty_midi\n",
        "!pip install pypianoroll\n",
        "!pip install mir_eval\n",
        "!apt install fluidsynth #Pip does not work for some reason. Only apt works\n",
        "!pip install midi2audio\n",
        "!git clone https://github.com/asigalov61/arc-diagrams\n",
        "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 /content/font.sf2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZajWNtE0Hwr"
      },
      "source": [
        "# Setup modules, functions, variables, and GPU check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj9Af0iFod6g"
      },
      "source": [
        "#@title Load all modules, check the available devices (GPU/CPU), and setup MIDI parameters\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import time\n",
        "\n",
        "import pretty_midi\n",
        "from midi2audio import FluidSynth\n",
        "from google.colab import output\n",
        "from IPython.display import display, Javascript, HTML, Audio\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import pypianoroll\n",
        "from pypianoroll import Multitrack, Track\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "#matplotlib.use('SVG')\n",
        "# For plotting\n",
        "import mir_eval.display\n",
        "import librosa.display\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from mido import MidiFile\n",
        "%cd /content/arc-diagrams/\n",
        "from arc_diagram import plot_arc_diagram\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
        "print('Available Device:', device)\n",
        "\n",
        "!mkdir /content/midis\n",
        "\n",
        "sample_freq_variable = 12 #@param {type:\"number\"}\n",
        "note_range_variable = 62 #@param {type:\"number\"}\n",
        "note_offset_variable = 33 #@param {type:\"number\"}\n",
        "number_of_instruments = 2 #@param {type:\"number\"}\n",
        "chamber_option = True #@param {type:\"boolean\"}\n",
        "\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiNnwNj80UwP"
      },
      "source": [
        "# (OPTIONS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EpQajTGqIKr",
        "cellView": "form"
      },
      "source": [
        "#@title (OPTION 1) Convert your own MIDIs to Notewise TXT DataSet (before running this cell, upload your MIDI DataSet to /content/midis folder)\n",
        "import tqdm.auto\n",
        "import argparse\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from math import floor\n",
        "from pyknon.genmidi import Midi\n",
        "from pyknon.music import NoteSeq, Note\n",
        "import music21\n",
        "from music21 import instrument, volume\n",
        "from music21 import midi as midiModule\n",
        "from pathlib import Path\n",
        "import glob, sys\n",
        "from music21 import converter, instrument\n",
        "%cd /content\n",
        "notes=[]\n",
        "InstrumentID=0\n",
        "folder = '/content/midis/*mid'\n",
        "for file in tqdm.auto.tqdm(glob.glob(folder)):\n",
        "    filename = file[-53:]\n",
        "    print(filename)\n",
        "\n",
        "    # fname = \"../midi-files/mozart/sonat-3.mid\"\n",
        "    fname = filename\n",
        "\n",
        "    mf=music21.midi.MidiFile()\n",
        "    mf.open(fname)\n",
        "    mf.read()\n",
        "    mf.close()\n",
        "    midi_stream=music21.midi.translate.midiFileToStream(mf)\n",
        "    midi_stream\n",
        "\n",
        "\n",
        "\n",
        "    sample_freq=sample_freq_variable\n",
        "    note_range=note_range_variable\n",
        "    note_offset=note_offset_variable\n",
        "    chamber=chamber_option\n",
        "    numInstruments=number_of_instruments\n",
        "\n",
        "    s = midi_stream\n",
        "    #print(s.duration.quarterLength)\n",
        "\n",
        "    s[0].elements\n",
        "\n",
        "\n",
        "    maxTimeStep = floor(s.duration.quarterLength * sample_freq)+1\n",
        "    score_arr = np.zeros((maxTimeStep, numInstruments, note_range))\n",
        "\n",
        "    #print(maxTimeStep, \"\\n\", score_arr.shape)\n",
        "\n",
        "    # define two types of filters because notes and chords have different structures for storing their data\n",
        "    # chord have an extra layer because it consist of multiple notes\n",
        "\n",
        "    noteFilter=music21.stream.filters.ClassFilter('Note')\n",
        "    chordFilter=music21.stream.filters.ClassFilter('Chord')\n",
        "\n",
        "\n",
        "    # pitch.midi-note_offset: pitch is the numerical representation of a note.\n",
        "    #                         note_offset is the the pitch relative to a zero mark. eg. B-=25, C=27, A=24\n",
        "\n",
        "    # n.offset: the timestamps of each note, relative to the start of the score\n",
        "    #           by multiplying with the sample_freq, you make all the timestamps integers\n",
        "\n",
        "    # n.duration.quarterLength: the duration of that note as a float eg. quarter note = 0.25, half note = 0.5\n",
        "    #                           multiply by sample_freq to represent duration in terms of timesteps\n",
        "\n",
        "    notes = []\n",
        "    instrumentID = 0\n",
        "    parts = instrument.partitionByInstrument(s)\n",
        "    for i in range(len(parts.parts)):\n",
        "      instru = parts.parts[i].getInstrument()\n",
        "\n",
        "\n",
        "    for n in s.recurse().addFilter(noteFilter):\n",
        "        if chamber:\n",
        "          # assign_instrument where 0 means piano-like and 1 means violin-like, and -1 means neither\n",
        "          if instru.instrumentName == 'Violin':\n",
        "            notes.append((n.pitch.midi-note_offset, floor(n.offset*sample_freq),\n",
        "              floor(n.duration.quarterLength*sample_freq), 1))\n",
        "\n",
        "        notes.append((n.pitch.midi-note_offset, floor(n.offset*sample_freq),\n",
        "              floor(n.duration.quarterLength*sample_freq), 0))\n",
        "\n",
        "    #print(len(notes))\n",
        "    notes[-5:]\n",
        "\n",
        "    # do the same using a chord filter\n",
        "\n",
        "    for c in s.recurse().addFilter(chordFilter):\n",
        "        # unlike the noteFilter, this line of code is necessary as there are multiple notes in each chord\n",
        "        # pitchesInChord is a list of notes at each chord eg. (<music21.pitch.Pitch D5>, <music21.pitch.Pitch F5>)\n",
        "        pitchesInChord=c.pitches\n",
        "\n",
        "        # do same as noteFilter and append all notes to the notes list\n",
        "        for p in pitchesInChord:\n",
        "            notes.append((p.midi-note_offset, floor(c.offset*sample_freq),\n",
        "                          floor(c.duration.quarterLength*sample_freq), 1))\n",
        "\n",
        "        # do same as noteFilter and append all notes to the notes list\n",
        "        for p in pitchesInChord:\n",
        "            notes.append((p.midi-note_offset, floor(c.offset*sample_freq),\n",
        "                          floor(c.duration.quarterLength*sample_freq), 0))\n",
        "    #print(len(notes))\n",
        "    notes[-5:]\n",
        "\n",
        "    # the variable/list \"notes\" is a collection of all the notes in the song, not ordered in any significant way\n",
        "\n",
        "    for n in notes:\n",
        "\n",
        "        # pitch is the first variable in n, previously obtained by n.midi-note_offset\n",
        "        pitch=n[0]\n",
        "\n",
        "        # do some calibration for notes that fall our of note range\n",
        "        # i.e. less than 0 or more than note_range\n",
        "        while pitch<0:\n",
        "            pitch+=12\n",
        "        while pitch>=note_range:\n",
        "            pitch-=12\n",
        "\n",
        "        # 3rd element refers to instrument type => if instrument is violin, use different pitch calibration\n",
        "        if n[3]==1:      #Violin lowest note is v22\n",
        "            while pitch<22:\n",
        "                pitch+=12\n",
        "\n",
        "        # start building the 3D-tensor of shape: (796, 1, 38)\n",
        "        # score_arr[0] = timestep\n",
        "        # score_arr[1] = type of instrument\n",
        "        # score_arr[2] = pitch/note out of the range of note eg. 38\n",
        "\n",
        "        # n[0] = pitch\n",
        "        # n[1] = timestep\n",
        "        # n[2] = duration\n",
        "        # n[3] = instrument\n",
        "        #print(n[3])\n",
        "        try:\n",
        "          score_arr[n[1], n[3], pitch]=1                  # Strike note\n",
        "          score_arr[n[1]+1:n[1]+n[2], n[3], pitch]=2      # Continue holding note\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "    #print(score_arr.shape)\n",
        "    # print first 5 timesteps\n",
        "    score_arr[:5,0,]\n",
        "\n",
        "\n",
        "    for timestep in score_arr:\n",
        "        #print(list(reversed(range(len(timestep)))))\n",
        "        break\n",
        "\n",
        "    instr={}\n",
        "    instr[0]=\"p\"\n",
        "    instr[1]=\"v\"\n",
        "\n",
        "    score_string_arr=[]\n",
        "\n",
        "    # loop through all timesteps\n",
        "    for timestep in score_arr:\n",
        "\n",
        "        # selecting the instruments: i=0 means piano and i=1 means violin\n",
        "        for i in list(reversed(range(len(timestep)))):   # List violin note first, then piano note\n",
        "\n",
        "            #\n",
        "            score_string_arr.append(instr[i]+''.join([str(int(note)) for note in timestep[i]]))\n",
        "\n",
        "    #print(type(score_string_arr), len(score_string_arr))\n",
        "    score_string_arr[:5]\n",
        "\n",
        "    modulated=[]\n",
        "    # get the note range from the array\n",
        "    note_range=len(score_string_arr[0])-1\n",
        "\n",
        "    for i in range(0,12):\n",
        "        for chord in score_string_arr:\n",
        "\n",
        "            # minus the instrument letter eg. 'p'\n",
        "            # add 6 zeros on each side of the string\n",
        "            padded='000000'+chord[1:]+'000000'\n",
        "\n",
        "            # add back the instrument letter eg. 'p'\n",
        "            # append window of len=note_range back into\n",
        "            # eg. if we have \"00012345000\"\n",
        "            # iteratively, we want to get \"p00012\", \"p00123\", \"p01234\", \"p12345\", \"p23450\", \"p34500\", \"p45000\",\n",
        "            modulated.append(chord[0]+padded[i:i+note_range])\n",
        "\n",
        "    # 796 * 12\n",
        "    #print(len(modulated))\n",
        "    modulated[:5]\n",
        "\n",
        "    # input of this function is a modulated string\n",
        "    long_string = modulated\n",
        "\n",
        "    translated_list=[]\n",
        "\n",
        "    # for every timestep of the string\n",
        "    for j in range(len(long_string)):\n",
        "\n",
        "        # chord at timestep j eg. 'p00000000000000000000000000000000000100'\n",
        "        chord=long_string[j]\n",
        "        next_chord=\"\"\n",
        "\n",
        "        # range is from next_timestep to max_timestep\n",
        "        for k in range(j+1, len(long_string)):\n",
        "\n",
        "            # checking if instrument of next chord is same as current chord\n",
        "            if long_string[k][0]==chord[0]:\n",
        "\n",
        "                # if same, set next chord as next element in modulation\n",
        "                # otherwise, keep going until you find a chord with the same instrument\n",
        "                # when you do, set it as the next chord\n",
        "                next_chord=long_string[k]\n",
        "                break\n",
        "\n",
        "        # set prefix as the instrument\n",
        "        # set chord and next_chord to be without the instrument prefix\n",
        "        # next_chord is necessary to check when notes end\n",
        "        prefix=chord[0]\n",
        "        chord=chord[1:]\n",
        "        next_chord=next_chord[1:]\n",
        "\n",
        "        # checking for non-zero notes at one particular timestep\n",
        "        # i is an integer indicating the index of each note the chord\n",
        "        for i in range(len(chord)):\n",
        "\n",
        "            if chord[i]==\"0\":\n",
        "                continue\n",
        "\n",
        "            # set note as 2 elements: instrument and index of note\n",
        "            # examples: p22, p16, p4\n",
        "            #p = music21.pitch.Pitch()\n",
        "            #nt = music21.note.Note(p)\n",
        "            #n.volume.velocity = 20\n",
        "            #nt.volume.client == nt\n",
        "            #V = nt.volume.velocity\n",
        "            #print(V)\n",
        "            #note=prefix+str(i)+' V' + str(V)\n",
        "            note=prefix+str(i)\n",
        "\n",
        "            # if note in chord is 1, then append the note eg. p22 to the list\n",
        "            if chord[i]==\"1\":\n",
        "                translated_list.append(note)\n",
        "\n",
        "            # If chord[i]==\"2\" do nothing - we're continuing to hold the note\n",
        "\n",
        "            # unless next_chord[i] is back to \"0\" and it's time to end the note.\n",
        "            if next_chord==\"\" or next_chord[i]==\"0\":\n",
        "                translated_list.append(\"end\"+note)\n",
        "\n",
        "        # wait indicates end of every timestep\n",
        "        if prefix==\"p\":\n",
        "            translated_list.append(\"wait\")\n",
        "\n",
        "    #print(len(translated_list))\n",
        "    translated_list[:10]\n",
        "\n",
        "    # this section transforms the list of notes into a string of notes\n",
        "\n",
        "    # initialize i as zero and empty string\n",
        "    i=0\n",
        "    translated_string=\"\"\n",
        "\n",
        "\n",
        "    while i<len(translated_list):\n",
        "\n",
        "        # stack all the repeated waits together using an integer to indicate the no. of waits\n",
        "        # eg. \"wait wait\" => \"wait2\"\n",
        "        wait_count=1\n",
        "        if translated_list[i]=='wait':\n",
        "            while wait_count<=sample_freq*2 and i+wait_count<len(translated_list) and translated_list[i+wait_count]=='wait':\n",
        "                wait_count+=1\n",
        "            translated_list[i]='wait'+str(wait_count)\n",
        "\n",
        "        # add next note\n",
        "        translated_string+=translated_list[i]+\" \"\n",
        "        i+=wait_count\n",
        "\n",
        "    translated_string[:100]\n",
        "    len(translated_string)\n",
        "\n",
        "    #print(\"chordwise encoding type and length:\", type(modulated), len(modulated))\n",
        "    #print(\"notewise encoding type and length:\", type(translated_string), len(translated_string))\n",
        "\n",
        "    # default settings: sample_freq=12, note_range=62\n",
        "\n",
        "    chordwise_folder = \"../\"\n",
        "    notewise_folder = \"../\"\n",
        "\n",
        "    # export chordwise encoding\n",
        "#    f=open(chordwise_folder+fname+\"_chordwise\"+\".txt\",\"w+\")\n",
        "#    f.write(\" \".join(modulated))\n",
        "#    f.close()\n",
        "\n",
        "    # export notewise encoding\n",
        "    f=open(notewise_folder+fname+\"_notewise\"+\".txt\",\"w+\")\n",
        "    f.write(translated_string)\n",
        "    f.close()\n",
        "\n",
        "folder = '/content/midis/*notewise.txt'\n",
        "\n",
        "\n",
        "filenames = glob.glob('/content')\n",
        "with open('notewise_custom_dataset.txt', 'w') as outfile:\n",
        "    for fname in glob.glob(folder)[-53:]:\n",
        "        with open(fname) as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)\n",
        "\n",
        "#folder = '/content/midis/*chordwise.txt'\n",
        "\n",
        "#filenames = glob.glob('/content')\n",
        "#with open('chordwise_custom_dataset.txt', 'w') as outfile:\n",
        "#    for fname in glob.glob(folder)[-53:]:\n",
        "#        with open(fname) as infile:\n",
        "#            for line in infile:\n",
        "#                outfile.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xqj0BSFwD2g",
        "cellView": "form"
      },
      "source": [
        "#@title (OPTION 2) Download ready-to-use Piano and Chamber Notewise DataSets\n",
        "%cd /content/\n",
        "!wget -nc 'https://github.com/asigalov61/SuperPiano/raw/master/Super%20Chamber%20Piano%20Violin%20Notewise%20DataSet.zip'\n",
        "!unzip -o '/content/Super Chamber Piano Violin Notewise DataSet.zip'\n",
        "!rm '/content/Super Chamber Piano Violin Notewise DataSet.zip'\n",
        "\n",
        "!wget -nc 'https://github.com/asigalov61/SuperPiano/raw/master/Super%20Chamber%20Piano%20Only%20Notewise%20DataSet.zip'\n",
        "!unzip -o '/content/Super Chamber Piano Only Notewise DataSet.zip'\n",
        "!rm '/content/Super Chamber Piano Only Notewise DataSet.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26ggEbRwod6p",
        "cellView": "form"
      },
      "source": [
        "#@title Load and Encode TXT Notes DataSet\n",
        "select_training_dataset_file = \"/content/notewise_chamber.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# replace with any text file containing full set of data\n",
        "MIDI_data = select_training_dataset_file\n",
        "\n",
        "with open(MIDI_data, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# get vocabulary set\n",
        "words = sorted(tuple(set(text.split())))\n",
        "n = len(words)\n",
        "\n",
        "# create word-integer encoder/decoder\n",
        "word2int = dict(zip(words, list(range(n))))\n",
        "int2word = dict(zip(list(range(n)), words))\n",
        "\n",
        "# encode all words in dataset into integers\n",
        "encoded = np.array([word2int[word] for word in text.split()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5gMiSE20Zgv"
      },
      "source": [
        "# Main Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESN36y_od6s",
        "cellView": "form"
      },
      "source": [
        "#@title Define all functions\n",
        "# define model using the pytorch nn module\n",
        "class WordLSTM(nn.ModuleList):\n",
        "\n",
        "    def __init__(self, sequence_len, vocab_size, hidden_dim, batch_size):\n",
        "        super(WordLSTM, self).__init__()\n",
        "\n",
        "        # init the hyperparameters\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sequence_len = sequence_len\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # first layer lstm cell\n",
        "        self.lstm_1 = nn.LSTMCell(input_size=vocab_size, hidden_size=hidden_dim)\n",
        "\n",
        "        # second layer lstm cell\n",
        "        self.lstm_2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim)\n",
        "\n",
        "        # third layer lstm cell\n",
        "        #self.lstm_3 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.35)\n",
        "\n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
        "\n",
        "    # forward pass in training\n",
        "    def forward(self, x, hc):\n",
        "        \"\"\"\n",
        "            accepts 2 arguments:\n",
        "            1. x: input of each batch\n",
        "                - shape 128*149 (batch_size*vocab_size)\n",
        "            2. hc: tuple of init hidden, cell states\n",
        "                - each of shape 128*512 (batch_size*hidden_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        # create empty output seq\n",
        "        output_seq = torch.empty((self.sequence_len,\n",
        "                                  self.batch_size,\n",
        "                                  self.vocab_size))\n",
        "        # if using gpu\n",
        "        output_seq = output_seq.to(device)\n",
        "\n",
        "        # init hidden, cell states for lstm layers\n",
        "        hc_1, hc_2, hc_3 = hc, hc, hc\n",
        "\n",
        "        # for t-th word in every sequence\n",
        "        for t in range(self.sequence_len):\n",
        "\n",
        "            # layer 1 lstm\n",
        "            hc_1 = self.lstm_1(x[t], hc_1)\n",
        "            h_1, c_1 = hc_1\n",
        "\n",
        "            # layer 2 lstm\n",
        "            hc_2 = self.lstm_2(h_1, hc_2)\n",
        "            h_2, c_2 = hc_2\n",
        "\n",
        "            # layer 3 lstm\n",
        "            #hc_3 = self.lstm_3(h_2, hc_3)\n",
        "            #h_3, c_3 = hc_3\n",
        "\n",
        "            # dropout and fully connected layer\n",
        "            output_seq[t] = self.fc(self.dropout(h_2))\n",
        "\n",
        "        return output_seq.view((self.sequence_len * self.batch_size, -1))\n",
        "\n",
        "    def init_hidden(self):\n",
        "\n",
        "        # initialize hidden, cell states for training\n",
        "        # if using gpu\n",
        "        return (torch.zeros(self.batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.batch_size, self.hidden_dim).to(device))\n",
        "\n",
        "    def init_hidden_generator(self):\n",
        "\n",
        "        # initialize hidden, cell states for prediction of 1 sequence\n",
        "        # if using gpu\n",
        "        return (torch.zeros(1, self.hidden_dim).to('cpu'),\n",
        "                torch.zeros(1, self.hidden_dim).to('cpu'))\n",
        "\n",
        "    def predict(self, seed_seq, top_k=5, pred_len=128):\n",
        "        \"\"\"\n",
        "            accepts 3 arguments:\n",
        "            1. seed_seq: seed string sequence for prediction (prompt)\n",
        "            2. top_k: top k words to sample prediction from\n",
        "            3. pred_len: number of words to generate after the seed seq\n",
        "        \"\"\"\n",
        "\n",
        "        # set evaluation mode\n",
        "        self.eval()\n",
        "\n",
        "        # split string into list of words\n",
        "        seed_seq = seed_seq.split()\n",
        "\n",
        "        # get seed sequence length\n",
        "        seed_len = len(seed_seq)\n",
        "\n",
        "        # create output sequence\n",
        "        out_seq = np.empty(seed_len+pred_len)\n",
        "\n",
        "        # append input seq to output seq\n",
        "        out_seq[:seed_len] = np.array([word2int[word] for word in seed_seq])\n",
        "\n",
        "        # init hidden, cell states for generation\n",
        "        hc = self.init_hidden_generator()\n",
        "        hc_1, hc_2, hc_3 = hc, hc, hc\n",
        "\n",
        "        # feed seed string into lstm\n",
        "        # get the hidden state set up\n",
        "        for word in seed_seq[:-1]:\n",
        "\n",
        "            # encode starting word to one-hot encoding\n",
        "            word = to_categorical(word2int[word], num_classes=self.vocab_size).float()\n",
        "\n",
        "            # add batch dimension\n",
        "            word = torch.from_numpy(word).unsqueeze(0)\n",
        "            # if using gpu\n",
        "            word = word.to('cpu')\n",
        "\n",
        "            # layer 1 lstm\n",
        "            hc_1 = self.lstm_1(word, hc_1)\n",
        "            h_1, c_1 = hc_1\n",
        "\n",
        "            # layer 2 lstm\n",
        "            hc_2 = self.lstm_2(h_1, hc_2)\n",
        "            h_2, c_2 = hc_2\n",
        "\n",
        "            # layer 3 lstm\n",
        "            #hc_3 = self.lstm_3(h_2, hc_3)\n",
        "            #h_3, c_3 = hc_3\n",
        "\n",
        "        word = seed_seq[-1]\n",
        "\n",
        "        # encode starting word to one-hot encoding\n",
        "        word = to_categorical(word2int[word], num_classes=self.vocab_size)\n",
        "\n",
        "        # add batch dimension\n",
        "        word = torch.from_numpy(word).unsqueeze(0).float()\n",
        "        # if using gpu\n",
        "        word = word.to('cpu')\n",
        "\n",
        "        # forward pass\n",
        "        for t in range(pred_len):\n",
        "\n",
        "            # layer 1 lstm\n",
        "            hc_1 = self.lstm_1(word, hc_1)\n",
        "            h_1, c_1 = hc_1\n",
        "\n",
        "            # layer 2 lstm\n",
        "            hc_2 = self.lstm_2(h_1, hc_2)\n",
        "            h_2, c_2 = hc_2\n",
        "\n",
        "            # layer 3 lstm\n",
        "            #hc_3 = self.lstm_3(h_2, hc_3)\n",
        "            #h_3, c_3 = hc_3\n",
        "\n",
        "            # fully connected layer without dropout (no need)\n",
        "            output = self.fc(h_2)\n",
        "\n",
        "            # software to get probabilities of output options\n",
        "            output = F.softmax(output, dim=1)\n",
        "\n",
        "            # get top k words and corresponding probabilities\n",
        "            p, top_word = output.topk(top_k)\n",
        "            # if using gpu\n",
        "            p = p.cpu()\n",
        "\n",
        "            # sample from top k words to get next word\n",
        "            p = p.detach().squeeze().numpy()\n",
        "            top_word = torch.squeeze(top_word)\n",
        "\n",
        "            word = np.random.choice(top_word, p = p/p.sum())\n",
        "\n",
        "            # add word to sequence\n",
        "            out_seq[seed_len+t] = word\n",
        "\n",
        "            # encode predicted word to one-hot encoding for next step\n",
        "            word = to_categorical(word, num_classes=self.vocab_size)\n",
        "            word = torch.from_numpy(word).unsqueeze(0).float()\n",
        "            # word = torch.from_numpy(word).unsqueeze(0)\n",
        "            # if using gpu\n",
        "            word = word.to('cpu')\n",
        "\n",
        "        return out_seq\n",
        "\n",
        "\n",
        "def get_batches(arr, n_seqs, n_words):\n",
        "    \"\"\"\n",
        "        create generator object that returns batches of input (x) and target (y).\n",
        "        x of each batch has shape 128*128*149 (batch_size*seq_len*vocab_size).\n",
        "\n",
        "        accepts 3 arguments:\n",
        "        1. arr: array of words from text data\n",
        "        2. n_seq: number of sequence in each batch (aka batch_size)\n",
        "        3. n_word: number of words in each sequence\n",
        "    \"\"\"\n",
        "\n",
        "    # compute total elements / dimension of each batch\n",
        "    batch_total = n_seqs * n_words\n",
        "\n",
        "    # compute total number of complete batches\n",
        "    n_batches = arr.size//batch_total\n",
        "\n",
        "    # chop array at the last full batch\n",
        "    arr = arr[: n_batches* batch_total]\n",
        "\n",
        "    # reshape array to matrix with rows = no. of seq in one batch\n",
        "    arr = arr.reshape((n_seqs, -1))\n",
        "\n",
        "    # for each n_words in every row of the dataset\n",
        "    for n in range(0, arr.shape[1], n_words):\n",
        "\n",
        "        # chop it vertically, to get the input sequences\n",
        "        x = arr[:, n:n+n_words]\n",
        "\n",
        "        # init y - target with shape same as x\n",
        "        y = np.zeros_like(x)\n",
        "\n",
        "        # targets obtained by shifting by one\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, n+n_words]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
        "\n",
        "        # yield function is like return, but creates a generator object\n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "051CS4d9od6x",
        "cellView": "form"
      },
      "source": [
        "#@title Compile the Model\n",
        "training_batch_size = 1024 #@param {type:\"slider\", min:0, max:1024, step:16}\n",
        "attention_span_in_tokens = 256 #@param {type:\"slider\", min:0, max:512, step:64}\n",
        "hidden_dimension_size = 256 #@param {type:\"slider\", min:0, max:512, step:64}\n",
        "test_validation_ratio = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "learning_rate = 0.001 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "# compile the network - sequence_len, vocab_size, hidden_dim, batch_size\n",
        "net = WordLSTM(sequence_len=attention_span_in_tokens, vocab_size=len(word2int), hidden_dim=hidden_dimension_size, batch_size=training_batch_size)\n",
        "# if using gpu\n",
        "net.to(device)\n",
        "\n",
        "# define the loss and the optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# split dataset into 90% train and 10% using index\n",
        "val_idx = int(len(encoded) * (1 - test_validation_ratio))\n",
        "train_data, val_data = encoded[:val_idx], encoded[val_idx:]\n",
        "\n",
        "# empty list for the validation losses\n",
        "val_losses = list()\n",
        "\n",
        "# empty list for the samples\n",
        "samples = list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSDf9uWJ0g8f"
      },
      "source": [
        "# (OPTIONS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVajNLR2od6z"
      },
      "source": [
        "#@title (OPTION 1) Train the Model\n",
        "number_of_training_epochs = 300 #@param {type:\"slider\", min:1, max:300, step:1}\n",
        "\n",
        "import tqdm\n",
        "\n",
        "# track time\n",
        "start_time = time.time()\n",
        "\n",
        "# declare seed sequence\n",
        "#seed_string = \"p47 p50 wait8 endp47 endp50 wait4 p47 p50 wait8 endp47 endp50\"\n",
        "\n",
        "# finally train the model\n",
        "for epoch in tqdm.tqdm(range(number_of_training_epochs)):\n",
        "\n",
        "    # init the hidden and cell states to zero\n",
        "    hc = net.init_hidden()\n",
        "\n",
        "    # (x, y) refers to one batch with index i, where x is input, y is target\n",
        "    for i, (x, y) in enumerate(get_batches(train_data, training_batch_size, hidden_dimension_size)):\n",
        "\n",
        "        # get the torch tensors from the one-hot of training data\n",
        "        # also transpose the axis for the training set and the targets\n",
        "        x_train = torch.from_numpy(to_categorical(x, num_classes=net.vocab_size).transpose([1, 0, 2])).to(dtype=torch.float)\n",
        "        targets = torch.from_numpy(y.T).type(torch.LongTensor)  # tensor of the target\n",
        "\n",
        "        # if using gpu\n",
        "        x_train = x_train.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # zero out the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get the output sequence from the input and the initial hidden and cell states\n",
        "        # calls forward function\n",
        "        output = net(x_train, hc)\n",
        "\n",
        "        # calculate the loss\n",
        "        # we need to calculate the loss across all batches, so we have to flat the targets tensor\n",
        "        loss = criterion(output, targets.contiguous().view(training_batch_size*hidden_dimension_size))\n",
        "\n",
        "        # calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters of the model\n",
        "        optimizer.step()\n",
        "\n",
        "        # track time\n",
        "\n",
        "        # feedback every 100 batches\n",
        "        if i % 100 == 0:\n",
        "\n",
        "            # initialize the validation hidden state and cell state\n",
        "            val_h, val_c = net.init_hidden()\n",
        "\n",
        "            for val_x, val_y in get_batches(val_data, training_batch_size, hidden_dimension_size):\n",
        "\n",
        "                # prepare the validation inputs and targets\n",
        "                val_x = torch.from_numpy(to_categorical(val_x).transpose([1, 0, 2]))\n",
        "                val_y = torch.from_numpy(val_y.T).type(torch.LongTensor).contiguous().view(training_batch_size*hidden_dimension_size)\n",
        "\n",
        "                # if using gpu\n",
        "                val_x = val_x.to(device)\n",
        "                val_y = val_y.to(device)\n",
        "\n",
        "                # get the validation output\n",
        "                #val_output = net(val_x, (val_h, val_c))\n",
        "\n",
        "                # get the validation loss\n",
        "                #val_loss = criterion(val_output, val_y)\n",
        "\n",
        "                # append the validation loss\n",
        "                #val_losses.append(val_loss.item())\n",
        "\n",
        "                # samples.append(''.join([int2char[int_] for int_ in net.predict(\"p33\", seq_len=1024)]))\n",
        "\n",
        "#            with open(\"../content\" + str(epoch) + \"_batch\" + str(i) + \".txt\", \"w\") as loss_file:\n",
        "#                loss_file.write(\"Epoch: {}, Batch: {}, Train Loss: {:.6f}, Validation Loss: {:.6f}\".format(epoch, i, loss.item(), val_loss.item()))\n",
        "\n",
        "#            with open(\"../content\" + str(epoch) + \"_batch\" + str(i) + \".txt\", \"w\") as outfile:\n",
        "#                outfile.write(' '.join([int2word[int_] for int_ in net.predict(seed_seq=seed_string, pred_len=512)]))\n",
        "\n",
        "            # track time\n",
        "            duration = round(time.time() - start_time, 1)\n",
        "            start_time = time.time()\n",
        "\n",
        "            print(\"Epoch: {}, Batch: {}, Duration: {} sec, Test Loss: {}\".format(epoch, i, duration, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rythm-awared Score Prediction Model"
      ],
      "metadata": {
        "id": "pZ_s6TG64uJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title My Original Time Decayed Cross-Entropy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# ===============================================================\n",
        "# Global Parameters & Device Setup\n",
        "# ===============================================================\n",
        "# Sequence lengths (from t=0) to calculate accuracy for.\n",
        "ACCURACY_SEQUENCE_LENGTHS = [1, 4, 16, 64, 128, 256, 512]\n",
        "# Frequency to save checkpoints\n",
        "save_checkpoint_every = 5\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ===============================================================\n",
        "# Custom Loss Function (No changes from original)\n",
        "# ===============================================================\n",
        "class TemporalWeightedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self,\n",
        "                 time_weight_type: str | None = 'linear',\n",
        "                 time_weight_gamma: float = 0.9,\n",
        "                 weight: torch.Tensor | None = None,\n",
        "                 ignore_index: int = -100,\n",
        "                 reduction: str = 'mean',\n",
        "                 label_smoothing: float = 0.0):\n",
        "        super().__init__()\n",
        "        if time_weight_type not in ['linear', 'exponential', None]:\n",
        "            raise ValueError(\"time_weight_type must be 'linear', 'exponential', or None\")\n",
        "        if reduction not in ['mean', 'sum']:\n",
        "            raise ValueError(\"reduction must be 'mean' or 'sum'\")\n",
        "        self.time_weight_type = time_weight_type\n",
        "        self.time_weight_gamma = time_weight_gamma\n",
        "        self.reduction = reduction\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            weight=weight,\n",
        "            ignore_index=ignore_index,\n",
        "            reduction='none',\n",
        "            label_smoothing=label_smoothing\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        if input.dim() == 3:\n",
        "            # input shape: [batch, seq_len, classes]\n",
        "            # target shape: [batch, seq_len]\n",
        "            # nn.CrossEntropyLoss expects input as [batch, classes, seq_len]\n",
        "            input_permuted = input.transpose(1, 2)\n",
        "            loss_per_step = self.criterion(input_permuted, target) # shape: [batch, seq_len]\n",
        "\n",
        "            if self.time_weight_type is None:\n",
        "                if self.reduction == 'mean': return loss_per_step.mean()\n",
        "                else: return loss_per_step.sum()\n",
        "\n",
        "            weights = self._get_weights(loss_per_step.shape[1], loss_per_step.device)\n",
        "            weighted_loss = loss_per_step * weights\n",
        "\n",
        "            if self.reduction == 'mean': return weighted_loss.mean()\n",
        "            else: return weighted_loss.sum()\n",
        "\n",
        "        elif input.dim() == 2:\n",
        "            warnings.warn(\n",
        "                \"TemporalWeightedCrossEntropyLoss received a 2D input. \"\n",
        "                \"Temporal weighting is being skipped. \"\n",
        "                \"Standard CrossEntropyLoss is applied instead.\"\n",
        "            )\n",
        "            loss_per_element = self.criterion(input, target)\n",
        "            if self.reduction == 'mean': return loss_per_element.mean()\n",
        "            else: return loss_per_element.sum()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported input dimension: {input.dim()}. Expected 2 or 3.\")\n",
        "\n",
        "    def _get_weights(self, seq_len: int, device: torch.device) -> torch.Tensor:\n",
        "        if self.time_weight_type == 'linear':\n",
        "            return torch.linspace(1.0, 0.1, seq_len, device=device)\n",
        "        elif self.time_weight_type == 'exponential':\n",
        "            return torch.tensor(\n",
        "                [self.time_weight_gamma**t for t in range(seq_len)],\n",
        "                device=device, dtype=torch.float32\n",
        "            )\n",
        "        else:\n",
        "            return torch.ones(seq_len, device=device)\n",
        "\n",
        "    def plot_weights(self, seq_len: int = 50, save_path: str | None = None):\n",
        "        weights = self._get_weights(seq_len, device='cpu').numpy()\n",
        "        timesteps = np.arange(seq_len)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(timesteps, weights, marker='o', linestyle='--')\n",
        "        title = f\"Weight Decay Function (type: {self.time_weight_type or 'none'}\"\n",
        "        if self.time_weight_type == 'exponential': title += f\", gamma: {self.time_weight_gamma}\"\n",
        "        title += \")\"\n",
        "        plt.title(title)\n",
        "        plt.xlabel(\"Time Step\")\n",
        "        plt.ylabel(\"Weight\")\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.ylim(0, 1.1)\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "            print(f\"Weight plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# --- Google Colabでの使用例 ---\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"--- 1. 線形減衰のグラフ ---\")\n",
        "loss_fn_linear = TemporalWeightedCrossEntropyLoss(time_weight_type='linear')\n",
        "# save_pathを指定しないことでインライン表示される\n",
        "loss_fn_linear.plot_weights(seq_len=64)\n",
        "\n",
        "\n",
        "print(\"\\n--- 2. 指数減衰のグラフ ---\")\n",
        "loss_fn_exp = TemporalWeightedCrossEntropyLoss(\n",
        "    time_weight_type='exponential',\n",
        "    time_weight_gamma=0.9\n",
        ")\n",
        "loss_fn_exp.plot_weights(seq_len=64)\n"
      ],
      "metadata": {
        "id": "HuDtP5bLe4V2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified rhythm-to-score prediction model with dual mode support\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import re\n",
        "import copy\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "# Global constants\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "save_checkpoint_every = 5\n",
        "ACCURACY_SEQUENCE_LENGTHS = [1, 4, 8, 16, 32, 64, 128, 256]\n",
        "\n",
        "# ===============================================================\n",
        "# Data Processing Functions\n",
        "# ===============================================================\n",
        "def extract_rhythm_from_score(score_string):\n",
        "    \"\"\"Extract rhythm information from score string by masking pitch information.\"\"\"\n",
        "    tokens = score_string.split()\n",
        "    rhythm_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.startswith('wait'):\n",
        "            rhythm_tokens.append(token)\n",
        "        elif token.startswith('end'):\n",
        "            if token[3] in ['p', 'v']:\n",
        "                rhythm_tokens.append(token[:4] + '<mask>')\n",
        "            else:\n",
        "                rhythm_tokens.append(token)\n",
        "        elif token[0] in ['p', 'v'] and len(token) > 1 and token[1:].isdigit():\n",
        "            rhythm_tokens.append(token[0] + '<mask>')\n",
        "        else:\n",
        "            rhythm_tokens.append(token)\n",
        "    return ' '.join(rhythm_tokens)\n",
        "\n",
        "def prepare_rhythm_to_score_data(text_data, sequence_length=512):\n",
        "    \"\"\"Prepare training data with overlapping sequences.\"\"\"\n",
        "    all_tokens = text_data.strip().split()\n",
        "    rhythm_sequences, score_sequences = [], []\n",
        "\n",
        "    # Ensure we have at least sequence_length + 1 tokens for proper training\n",
        "    for i in range(0, len(all_tokens) - sequence_length, sequence_length // 2):\n",
        "        # Extract sequence_length + 1 tokens to allow for (r(t+1), x(t)) -> x(t+1) mapping\n",
        "        score_chunk = all_tokens[i:i + sequence_length + 1]\n",
        "        score_text = ' '.join(score_chunk)\n",
        "        rhythm_text = extract_rhythm_from_score(score_text)\n",
        "        rhythm_sequences.append(rhythm_text)\n",
        "        score_sequences.append(score_text)\n",
        "\n",
        "    print(f\"Created {len(rhythm_sequences)} training sequences\")\n",
        "    return rhythm_sequences, score_sequences\n",
        "\n",
        "def prepare_vocabularies(rhythm_sequences, score_sequences, use_rhythm=True):\n",
        "    \"\"\"Create vocabularies with special tokens.\"\"\"\n",
        "    # Add special tokens - padding must be at index 0 for CrossEntropyLoss ignore_index\n",
        "    special_tokens = ['<pad>', '<unk>']\n",
        "\n",
        "    score_tokens = set(token for seq in score_sequences for token in seq.split())\n",
        "    score_tokens = special_tokens + sorted(list(score_tokens))\n",
        "\n",
        "    score2int = {token: i for i, token in enumerate(score_tokens)}\n",
        "    int2score = {i: token for i, token in enumerate(score_tokens)}\n",
        "\n",
        "    if use_rhythm:\n",
        "        rhythm_tokens = set(token for seq in rhythm_sequences for token in seq.split())\n",
        "        rhythm_tokens = special_tokens + sorted(list(rhythm_tokens))\n",
        "        rhythm2int = {token: i for i, token in enumerate(rhythm_tokens)}\n",
        "        int2rhythm = {i: token for i, token in enumerate(rhythm_tokens)}\n",
        "    else:\n",
        "        rhythm2int = None\n",
        "        int2rhythm = None\n",
        "\n",
        "    return rhythm2int, int2rhythm, score2int, int2score\n",
        "\n",
        "# ===============================================================\n",
        "# Modified Model Class with Dual Mode Support\n",
        "# ===============================================================\n",
        "class RhythmToScoreLSTM(nn.Module):\n",
        "    def __init__(self, rhythm_vocab_size, score_vocab_size, hidden_dim, num_layers=2, use_rhythm=True):\n",
        "        super(RhythmToScoreLSTM, self).__init__()\n",
        "        self.rhythm_vocab_size = rhythm_vocab_size\n",
        "        self.score_vocab_size = score_vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.use_rhythm = use_rhythm\n",
        "\n",
        "        # Input dimension depends on mode\n",
        "        if use_rhythm:\n",
        "            self.input_dim = rhythm_vocab_size + score_vocab_size\n",
        "        else:\n",
        "            self.input_dim = score_vocab_size\n",
        "\n",
        "        # LSTM takes one-hot vectors directly\n",
        "        self.lstm = nn.LSTM(self.input_dim, hidden_dim, num_layers,\n",
        "                           batch_first=True, dropout=0.35 if num_layers > 1 else 0)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_dim, score_vocab_size)\n",
        "        self.dropout = nn.Dropout(0.35)\n",
        "\n",
        "    def forward(self, rhythm_input, score_input, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass with optional rhythm input.\n",
        "\n",
        "        Args:\n",
        "            rhythm_input: [batch_size, seq_len] - indices for r(t+1) (ignored if use_rhythm=False)\n",
        "            score_input: [batch_size, seq_len] - indices for x(t)\n",
        "            hidden: LSTM hidden state\n",
        "\n",
        "        Returns:\n",
        "            output: [batch_size, seq_len, score_vocab_size] - predictions for x(t+1)\n",
        "            hidden: Updated LSTM hidden state\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = score_input.shape\n",
        "\n",
        "        # Convert to one-hot encodings\n",
        "        score_onehot = F.one_hot(score_input, num_classes=self.score_vocab_size).float()\n",
        "\n",
        "        if self.use_rhythm:\n",
        "            rhythm_onehot = F.one_hot(rhythm_input, num_classes=self.rhythm_vocab_size).float()\n",
        "            # Concatenate rhythm and score one-hot vectors\n",
        "            combined_input = torch.cat([rhythm_onehot, score_onehot], dim=-1)\n",
        "        else:\n",
        "            # Use only score one-hot vectors\n",
        "            combined_input = score_onehot\n",
        "\n",
        "        # Pass through LSTM\n",
        "        if hidden is not None:\n",
        "            lstm_out, hidden = self.lstm(combined_input, hidden)\n",
        "        else:\n",
        "            lstm_out, hidden = self.lstm(combined_input)\n",
        "\n",
        "        # Apply dropout and output layer\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        output = self.fc(lstm_out)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialize LSTM hidden state.\"\"\"\n",
        "        h = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return (h, c)\n",
        "\n",
        "    def generate(self, initial_score_token, score2int, int2score, rhythm_sequence=None,\n",
        "                 rhythm2int=None, max_length=512, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Generate a score sequence.\n",
        "\n",
        "        Args:\n",
        "            initial_score_token: Initial score token to start generation\n",
        "            score2int, int2score: Score vocabulary mappings\n",
        "            rhythm_sequence: String of rhythm tokens (required if use_rhythm=True)\n",
        "            rhythm2int: Rhythm vocabulary mapping (required if use_rhythm=True)\n",
        "            max_length: Maximum generation length\n",
        "            temperature: Sampling temperature\n",
        "\n",
        "        Returns:\n",
        "            Generated score sequence as string\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        if self.use_rhythm:\n",
        "            if rhythm_sequence is None or rhythm2int is None:\n",
        "                raise ValueError(\"rhythm_sequence and rhythm2int are required when use_rhythm=True\")\n",
        "\n",
        "            rhythm_tokens = rhythm_sequence.split()\n",
        "            if len(rhythm_tokens) <= 1:\n",
        "                return \"\"\n",
        "\n",
        "            # Determine generation length based on rhythm sequence\n",
        "            max_length = min(max_length, len(rhythm_tokens) - 1)\n",
        "\n",
        "        # Initialize with the initial score token\n",
        "        current_score_idx = score2int.get(initial_score_token, score2int['<unk>'])\n",
        "        generated_tokens = []\n",
        "\n",
        "        hidden = self.init_hidden(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_length):\n",
        "                # Prepare score input\n",
        "                score_input = torch.tensor([[current_score_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "                if self.use_rhythm:\n",
        "                    # Get r(t+1)\n",
        "                    rhythm_idx = rhythm2int.get(rhythm_tokens[i+1], rhythm2int['<unk>'])\n",
        "                    rhythm_input = torch.tensor([[rhythm_idx]], dtype=torch.long).to(device)\n",
        "                else:\n",
        "                    # Dummy rhythm input (not used in forward pass)\n",
        "                    rhythm_input = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                output, hidden = self.forward(rhythm_input, score_input, hidden)\n",
        "\n",
        "                # Apply temperature and sample\n",
        "                output = output.squeeze(0).squeeze(0) / temperature\n",
        "                probs = F.softmax(output, dim=-1)\n",
        "                current_score_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "                # Add to generated sequence\n",
        "                generated_tokens.append(int2score.get(current_score_idx, '<unk>'))\n",
        "\n",
        "        return ' '.join(generated_tokens)\n",
        "\n",
        "# ===============================================================\n",
        "# Training Function with Dual Mode Support\n",
        "# ===============================================================\n",
        "def train_rhythm_to_score_model(model, rhythm_sequences, score_sequences,\n",
        "                                rhythm2int, score2int, int2rhythm, int2score, epochs,\n",
        "                                learning_rate=0.001, batch_size=32, max_seq_length=512,\n",
        "                                start_epoch=0, optimizer_state=None, save_checkpoints=True,\n",
        "                                use_rhythm=True):\n",
        "    \"\"\"Train the model in either rhythm+score or score-only mode.\"\"\"\n",
        "\n",
        "    mode_str = \"rhythm_score\" if use_rhythm else \"score_only\"\n",
        "    log_dir = f\"runs/{mode_str}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    writer = SummaryWriter(log_dir)\n",
        "    print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    if optimizer_state is not None:\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "        print(\"Optimizer state loaded from checkpoint\")\n",
        "\n",
        "    # Use standard CrossEntropyLoss with ignore_index for padding\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is padding index\n",
        "\n",
        "    # Prepare dataset with proper alignment\n",
        "    dataset = []\n",
        "    pad_idx_score = score2int['<pad>']\n",
        "    pad_idx_rhythm = rhythm2int['<pad>'] if use_rhythm else 0\n",
        "\n",
        "    for rhythm_seq, score_seq in zip(rhythm_sequences, score_sequences):\n",
        "        score_tokens = score_seq.split()\n",
        "\n",
        "        if use_rhythm:\n",
        "            rhythm_tokens = rhythm_seq.split()\n",
        "            if len(rhythm_tokens) != len(score_tokens):\n",
        "                continue\n",
        "\n",
        "        # We need at least 2 tokens to create one training example\n",
        "        if len(score_tokens) < 2:\n",
        "            continue\n",
        "\n",
        "        if len(score_tokens) > max_seq_length + 1:\n",
        "            score_tokens = score_tokens[:max_seq_length + 1]\n",
        "            if use_rhythm:\n",
        "                rhythm_tokens = rhythm_tokens[:max_seq_length + 1]\n",
        "\n",
        "        # Convert to indices\n",
        "        score_indices = [score2int.get(token, score2int['<unk>']) for token in score_tokens]\n",
        "\n",
        "        if use_rhythm:\n",
        "            rhythm_indices = [rhythm2int.get(token, rhythm2int['<unk>']) for token in rhythm_tokens]\n",
        "            dataset.append((rhythm_indices, score_indices))\n",
        "        else:\n",
        "            dataset.append((None, score_indices))\n",
        "\n",
        "    print(f\"Total training samples after filtering: {len(dataset)}\")\n",
        "    if len(dataset) == 0:\n",
        "        print(\"No valid training samples found!\")\n",
        "        return\n",
        "\n",
        "    model.train()\n",
        "    pbar = tqdm.tqdm(range(start_epoch, epochs), desc=\"Training Progress\", unit=\"epoch\")\n",
        "    global_step = start_epoch * (len(dataset) // batch_size)\n",
        "\n",
        "    for epoch in pbar:\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        random.shuffle(dataset)\n",
        "\n",
        "        for i in range(0, len(dataset), batch_size):\n",
        "            batch_data = dataset[i:i+batch_size]\n",
        "\n",
        "            if use_rhythm:\n",
        "                # Find max length in batch (excluding last token for alignment)\n",
        "                max_len = max(len(rhythm) - 1 for rhythm, _ in batch_data)\n",
        "            else:\n",
        "                max_len = max(len(score) - 1 for _, score in batch_data)\n",
        "\n",
        "            if max_len == 0:\n",
        "                continue\n",
        "\n",
        "            batch_rhythm_input = []\n",
        "            batch_score_input = []\n",
        "            batch_score_target = []\n",
        "\n",
        "            for rhythm_indices, score_indices in batch_data:\n",
        "                seq_len = len(score_indices) - 1  # Number of training steps\n",
        "\n",
        "                if use_rhythm:\n",
        "                    # Create input sequences: r(t+1) and x(t)\n",
        "                    rhythm_input = rhythm_indices[1:seq_len+1]  # r(1) to r(seq_len)\n",
        "                    score_input = score_indices[:seq_len]        # x(0) to x(seq_len-1)\n",
        "                    score_target = score_indices[1:seq_len+1]    # x(1) to x(seq_len)\n",
        "\n",
        "                    # Pad sequences to max_len\n",
        "                    pad_len = max_len - len(rhythm_input)\n",
        "                    rhythm_input = rhythm_input + [pad_idx_rhythm] * pad_len\n",
        "                else:\n",
        "                    # Score-only mode: x(t) -> x(t+1)\n",
        "                    score_input = score_indices[:seq_len]        # x(0) to x(seq_len-1)\n",
        "                    score_target = score_indices[1:seq_len+1]    # x(1) to x(seq_len)\n",
        "                    rhythm_input = [0] * max_len  # Dummy rhythm input\n",
        "\n",
        "                # Pad score sequences\n",
        "                pad_len = max_len - len(score_input)\n",
        "                score_input = score_input + [pad_idx_score] * pad_len\n",
        "                score_target = score_target + [pad_idx_score] * pad_len\n",
        "\n",
        "                batch_rhythm_input.append(rhythm_input)\n",
        "                batch_score_input.append(score_input)\n",
        "                batch_score_target.append(score_target)\n",
        "\n",
        "            # Convert to tensors\n",
        "            rhythm_input_tensor = torch.tensor(batch_rhythm_input, dtype=torch.long).to(device)\n",
        "            score_input_tensor = torch.tensor(batch_score_input, dtype=torch.long).to(device)\n",
        "            score_target_tensor = torch.tensor(batch_score_target, dtype=torch.long).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                # Forward pass\n",
        "                output, _ = model(rhythm_input_tensor, score_input_tensor)\n",
        "\n",
        "                # Reshape for loss calculation\n",
        "                output = output.reshape(-1, model.score_vocab_size)\n",
        "                target = score_target_tensor.reshape(-1)\n",
        "\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "                global_step += 1\n",
        "                writer.add_scalar('Loss/batch', loss.item(), global_step)\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error in batch {i//batch_size}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if num_batches > 0:\n",
        "            avg_loss = total_loss / num_batches\n",
        "            writer.add_scalar('Loss/epoch', avg_loss, epoch)\n",
        "\n",
        "            # Calculate and log accuracy\n",
        "            accuracies = calculate_accuracy(\n",
        "                model, dataset, rhythm2int, score2int, device,\n",
        "                max_samples=500,\n",
        "                sequence_lengths=ACCURACY_SEQUENCE_LENGTHS,\n",
        "                use_rhythm=use_rhythm\n",
        "            )\n",
        "\n",
        "            # Log accuracies to TensorBoard\n",
        "            for seq_len, acc in accuracies.items():\n",
        "                writer.add_scalar(f'Accuracy/val_seq_{seq_len}', acc, epoch)\n",
        "\n",
        "            # Update progress bar\n",
        "            postfix_dict = {'Loss': f'{avg_loss:.4f}'}\n",
        "            display_accs = {f'Acc@{L}': f'{accuracies.get(L, 0):.3f}'\n",
        "                           for L in [1, 16, 64, 256] if L in accuracies}\n",
        "            postfix_dict.update(display_accs)\n",
        "            pbar.set_postfix(postfix_dict)\n",
        "\n",
        "        if save_checkpoints and (epoch + 1) % save_checkpoint_every == 0:\n",
        "            save_checkpoint(model, rhythm2int, int2rhythm, score2int, int2score,\n",
        "                          optimizer, epoch, avg_loss, use_rhythm=use_rhythm)\n",
        "\n",
        "    pbar.close()\n",
        "    writer.close()\n",
        "\n",
        "    if save_checkpoints:\n",
        "        final_loss = avg_loss if 'avg_loss' in locals() else 0\n",
        "        save_checkpoint(model, rhythm2int, int2rhythm, score2int, int2score,\n",
        "                       optimizer, epochs - 1, final_loss, use_rhythm=use_rhythm)\n",
        "\n",
        "# ===============================================================\n",
        "# Accuracy Calculation Function with Dual Mode Support\n",
        "# ===============================================================\n",
        "def calculate_accuracy(model, dataset, rhythm2int, score2int, device,\n",
        "                      max_samples=500, sequence_lengths=None, use_rhythm=True):\n",
        "    \"\"\"Calculate accuracy for different sequence lengths.\"\"\"\n",
        "    if sequence_lengths is None:\n",
        "        sequence_lengths = ACCURACY_SEQUENCE_LENGTHS\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    pad_idx_score = score2int['<pad>']\n",
        "\n",
        "    correct_counts = {L: 0 for L in sequence_lengths}\n",
        "    total_counts = {L: 0 for L in sequence_lengths}\n",
        "\n",
        "    sample_size = min(len(dataset), max_samples)\n",
        "    if sample_size == 0:\n",
        "        return {L: 0 for L in sequence_lengths}\n",
        "\n",
        "    sampled_data = random.sample(dataset, sample_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for rhythm_indices, score_indices in sampled_data:\n",
        "            if len(score_indices) < 2:\n",
        "                continue\n",
        "\n",
        "            seq_len = len(score_indices) - 1\n",
        "\n",
        "            # Prepare inputs\n",
        "            if use_rhythm:\n",
        "                rhythm_input = rhythm_indices[1:seq_len+1]\n",
        "            else:\n",
        "                rhythm_input = [0] * seq_len  # Dummy rhythm input\n",
        "\n",
        "            score_input = score_indices[:seq_len]\n",
        "            score_target = score_indices[1:seq_len+1]\n",
        "\n",
        "            # Convert to tensors (batch size = 1)\n",
        "            rhythm_tensor = torch.tensor([rhythm_input], dtype=torch.long).to(device)\n",
        "            score_tensor = torch.tensor([score_input], dtype=torch.long).to(device)\n",
        "            target_tensor = torch.tensor([score_target], dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output, _ = model(rhythm_tensor, score_tensor)\n",
        "            _, predicted = torch.max(output, dim=2)\n",
        "\n",
        "            # Remove batch dimension\n",
        "            predicted = predicted.squeeze(0)\n",
        "            target = target_tensor.squeeze(0)\n",
        "\n",
        "            # Calculate accuracy for each sequence length\n",
        "            for L in sequence_lengths:\n",
        "                eval_len = min(L, seq_len)\n",
        "                if eval_len == 0:\n",
        "                    continue\n",
        "\n",
        "                # Get predictions and targets for first L tokens\n",
        "                pred_slice = predicted[:eval_len]\n",
        "                target_slice = target[:eval_len]\n",
        "\n",
        "                # Count correct predictions (excluding padding)\n",
        "                mask = target_slice != pad_idx_score\n",
        "                if mask.sum() > 0:  # Only count if there are non-padding tokens\n",
        "                    correct_counts[L] += (pred_slice[mask] == target_slice[mask]).sum().item()\n",
        "                    total_counts[L] += mask.sum().item()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Calculate final accuracies\n",
        "    accuracies = {L: correct_counts[L] / total_counts[L] if total_counts[L] > 0 else 0\n",
        "                  for L in sequence_lengths}\n",
        "\n",
        "    return accuracies\n",
        "\n",
        "# ===============================================================\n",
        "# Checkpoint Functions with Mode Support\n",
        "# ===============================================================\n",
        "def save_checkpoint(model, rhythm2int, int2rhythm, score2int, int2score,\n",
        "                    optimizer, epoch, loss, checkpoint_dir=\"checkpoints\", use_rhythm=True):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    mode_str = \"rhythm_score\" if use_rhythm else \"score_only\"\n",
        "    checkpoint_dir = os.path.join(checkpoint_dir, mode_str)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'rhythm2int': rhythm2int,\n",
        "        'int2rhythm': int2rhythm,\n",
        "        'score2int': score2int,\n",
        "        'int2score': int2score,\n",
        "        'use_rhythm': use_rhythm,\n",
        "        'model_config': {\n",
        "            'rhythm_vocab_size': model.rhythm_vocab_size,\n",
        "            'score_vocab_size': model.score_vocab_size,\n",
        "            'hidden_dim': model.hidden_dim,\n",
        "            'num_layers': model.num_layers,\n",
        "            'use_rhythm': use_rhythm\n",
        "        }\n",
        "    }\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    latest_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pt')\n",
        "    torch.save(checkpoint, latest_path)\n",
        "\n",
        "    print(f\"\\nCheckpoint saved: {checkpoint_path}\")\n",
        "    return checkpoint_path\n",
        "\n",
        "def load_checkpoint(checkpoint_path, device):\n",
        "    \"\"\"Load model checkpoint.\"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    config = checkpoint['model_config']\n",
        "\n",
        "    use_rhythm = config.get('use_rhythm', True)  # Default to True for backward compatibility\n",
        "\n",
        "    model = RhythmToScoreLSTM(\n",
        "        rhythm_vocab_size=config['rhythm_vocab_size'],\n",
        "        score_vocab_size=config['score_vocab_size'],\n",
        "        hidden_dim=config['hidden_dim'],\n",
        "        num_layers=config['num_layers'],\n",
        "        use_rhythm=use_rhythm\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    rhythm2int = checkpoint['rhythm2int']\n",
        "    int2rhythm = checkpoint['int2rhythm']\n",
        "    score2int = checkpoint['score2int']\n",
        "    int2score = checkpoint['int2score']\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    print(f\"Checkpoint loaded from epoch {checkpoint['epoch']}. Last loss: {checkpoint['loss']:.4f}\")\n",
        "    print(f\"Model mode: {'rhythm+score' if use_rhythm else 'score-only'}\")\n",
        "\n",
        "    return model, rhythm2int, int2rhythm, score2int, int2score, checkpoint, start_epoch\n",
        "\n",
        "# ===============================================================\n",
        "# Main Training Pipeline with Mode Selection\n",
        "# ===============================================================\n",
        "def rhythm_to_score_training_pipeline(\n",
        "    use_rhythm=True,  # NEW: Toggle between rhythm+score and score-only modes\n",
        "    resume_from_checkpoint=False,\n",
        "    checkpoint_path=None,\n",
        "    select_training_dataset_file=\"path/to/your/data.txt\",\n",
        "    number_of_training_epochs=50,\n",
        "    attention_span_in_tokens=128,\n",
        "    training_batch_size=128,\n",
        "    hidden_dimension_size=256,\n",
        "    learning_rate=0.001\n",
        "):\n",
        "    \"\"\"\n",
        "    Main pipeline to orchestrate the training process.\n",
        "\n",
        "    Args:\n",
        "        use_rhythm: If True, use rhythm+score mode. If False, use score-only mode.\n",
        "        Other args remain the same...\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Training mode: {'rhythm+score' if use_rhythm else 'score-only'}\")\n",
        "\n",
        "    if resume_from_checkpoint and checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            model, rhythm2int, int2rhythm, score2int, int2score, checkpoint, start_epoch = \\\n",
        "                load_checkpoint(checkpoint_path, device)\n",
        "\n",
        "            # Verify mode consistency\n",
        "            loaded_use_rhythm = checkpoint.get('use_rhythm', True)\n",
        "            if loaded_use_rhythm != use_rhythm:\n",
        "                print(f\"Warning: Loaded model was trained in {'rhythm+score' if loaded_use_rhythm else 'score-only'} mode, \"\n",
        "                      f\"but requested mode is {'rhythm+score' if use_rhythm else 'score-only'}. Using loaded mode.\")\n",
        "                use_rhythm = loaded_use_rhythm\n",
        "\n",
        "            with open(select_training_dataset_file, 'r', encoding='utf-8') as file:\n",
        "                text_data = file.read()\n",
        "\n",
        "            sequence_length = min(attention_span_in_tokens, 1024)\n",
        "            rhythm_sequences, score_sequences = prepare_rhythm_to_score_data(text_data, sequence_length)\n",
        "\n",
        "            if len(rhythm_sequences) == 0:\n",
        "                print(\"Error: No training sequences created.\")\n",
        "                return\n",
        "\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "            actual_batch_size = min(training_batch_size, len(rhythm_sequences))\n",
        "\n",
        "            train_rhythm_to_score_model(\n",
        "                model, rhythm_sequences, score_sequences,\n",
        "                rhythm2int, score2int, int2rhythm, int2score,\n",
        "                epochs=number_of_training_epochs,\n",
        "                learning_rate=learning_rate, batch_size=actual_batch_size,\n",
        "                max_seq_length=sequence_length, start_epoch=start_epoch,\n",
        "                optimizer_state=checkpoint['optimizer_state_dict'],\n",
        "                use_rhythm=use_rhythm\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\\nStarting fresh training instead...\")\n",
        "            resume_from_checkpoint = False\n",
        "\n",
        "    if not resume_from_checkpoint:\n",
        "        with open(select_training_dataset_file, 'r', encoding='utf-8') as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "        print(\"Preparing rhythm-score pairs...\")\n",
        "        sequence_length = min(attention_span_in_tokens, 1024)\n",
        "        rhythm_sequences, score_sequences = prepare_rhythm_to_score_data(text_data, sequence_length)\n",
        "\n",
        "        if len(rhythm_sequences) == 0:\n",
        "            print(\"Error: No training sequences created.\")\n",
        "            return\n",
        "\n",
        "        rhythm2int, int2rhythm, score2int, int2score = prepare_vocabularies(\n",
        "            rhythm_sequences, score_sequences, use_rhythm=use_rhythm\n",
        "        )\n",
        "\n",
        "        if use_rhythm:\n",
        "            print(f\"Rhythm vocabulary size: {len(rhythm2int)}\")\n",
        "        print(f\"Score vocabulary size: {len(score2int)}\")\n",
        "\n",
        "        actual_batch_size = min(training_batch_size, len(rhythm_sequences))\n",
        "\n",
        "        # Set rhythm_vocab_size to 1 if not using rhythm (dummy value)\n",
        "        rhythm_vocab_size = len(rhythm2int) if use_rhythm else 1\n",
        "\n",
        "        model = RhythmToScoreLSTM(\n",
        "            rhythm_vocab_size=rhythm_vocab_size,\n",
        "            score_vocab_size=len(score2int),\n",
        "            hidden_dim=hidden_dimension_size,\n",
        "            num_layers=2,\n",
        "            use_rhythm=use_rhythm\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"Model initialized on {device}\")\n",
        "        print(f\"Starting training for {number_of_training_epochs} epochs...\")\n",
        "        print(f\"Batch size: {actual_batch_size}\")\n",
        "\n",
        "        train_rhythm_to_score_model(\n",
        "            model, rhythm_sequences, score_sequences,\n",
        "            rhythm2int, score2int, int2rhythm, int2score,\n",
        "            epochs=number_of_training_epochs,\n",
        "            learning_rate=learning_rate, batch_size=actual_batch_size,\n",
        "            max_seq_length=sequence_length, start_epoch=0,\n",
        "            use_rhythm=use_rhythm\n",
        "        )\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    # Return the model and vocabularies\n",
        "    return model, rhythm2int, int2rhythm, score2int, int2score\n"
      ],
      "metadata": {
        "id": "pPpUkNXw6ajS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "TE0Ub2IS77EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/"
      ],
      "metadata": {
        "id": "cTbvTlw9JZpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyperparameter and Training (non-Rhythm)\n",
        "\n",
        "number_of_training_epochs = 500 #@param {type:\"slider\", min:1, max:3000, step:1}\n",
        "resume_from_checkpoint = False #@param {type:\"boolean\"}\n",
        "checkpoint_path        = \"\" #@param {type:\"string\"}\n",
        "save_checkpoint_every  = 10 #@param {type:\"slider\", min:1, max:100, step:10}\n",
        "attention_span_in_tokens = 512 #@param {type:\"slider\", min:1, max:512, step:1}\n",
        "training_batch_size    = 1024 #@param {type:\"slider\", min:1, max:1024, step:1}\n",
        "hidden_dimension_size  = 256 #@param {type:\"slider\", min:1, max:512, step:1}\n",
        "learning_rate          = 0.005 #@param {type:\"number\"}\n",
        "use_rhythm             = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "model, rhythm2int, int2rhythm, score2int, int2score = rhythm_to_score_training_pipeline(\n",
        "    use_rhythm=use_rhythm,  # Use only previous scores\n",
        "    resume_from_checkpoint=resume_from_checkpoint,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    select_training_dataset_file=select_training_dataset_file,\n",
        "    number_of_training_epochs=number_of_training_epochs,\n",
        "    attention_span_in_tokens=attention_span_in_tokens,\n",
        "    training_batch_size=training_batch_size,\n",
        "    hidden_dimension_size=hidden_dimension_size,\n",
        "    learning_rate=learning_rate\n",
        ")"
      ],
      "metadata": {
        "id": "cu22mnSZ755f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyperparameter and Training (non-Rhythm)\n",
        "\n",
        "number_of_training_epochs = 500 #@param {type:\"slider\", min:1, max:3000, step:1}\n",
        "resume_from_checkpoint = False #@param {type:\"boolean\"}\n",
        "checkpoint_path        = \"\" #@param {type:\"string\"}\n",
        "save_checkpoint_every  = 100 #@param {type:\"slider\", min:1, max:100, step:10}\n",
        "attention_span_in_tokens = 512 #@param {type:\"slider\", min:1, max:512, step:1}\n",
        "training_batch_size    = 1024 #@param {type:\"slider\", min:1, max:1024, step:1}\n",
        "hidden_dimension_size  = 256 #@param {type:\"slider\", min:1, max:512, step:1}\n",
        "learning_rate          = 0.005 #@param {type:\"number\"}\n",
        "use_rhythm             = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "model, rhythm2int, int2rhythm, score2int, int2score = rhythm_to_score_training_pipeline(\n",
        "    use_rhythm=use_rhythm,  # Use only previous scores\n",
        "    resume_from_checkpoint=resume_from_checkpoint,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    select_training_dataset_file=select_training_dataset_file,\n",
        "    number_of_training_epochs=number_of_training_epochs,\n",
        "    attention_span_in_tokens=attention_span_in_tokens,\n",
        "    training_batch_size=training_batch_size,\n",
        "    hidden_dimension_size=hidden_dimension_size,\n",
        "    learning_rate=learning_rate\n",
        ")"
      ],
      "metadata": {
        "id": "DhC5z_XEdL_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload Tensorboard logs\n",
        "!tensorboard dev upload --logdir logs \\\n",
        "  --name \"Rhythm-awared Following Score Prediction Model\" \\\n",
        "  --description \"リズム入力を考慮した後続楽譜推定モデル\""
      ],
      "metadata": {
        "id": "BZpMyTYiwnH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload checkpoint & runs\n",
        "!zip -r /content/runs.zip /content/runs\n",
        "!zip -r /content/checkpoints.zip /content/checkpoints\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/runs.zip /content/checkpoints.zip /content/drive/MyDrive/\n",
        "!rm /content/runs.zip /content/checkpoints.zip"
      ],
      "metadata": {
        "id": "ygkCCQcSzoTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Rythm2Score model"
      ],
      "metadata": {
        "id": "CtuqjvtLMQbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate TXT and MIDI file\n",
        "prompt = \"wait9 p<mask> p<mask> p<mask> p<mask> wait4 endp<mask>\" #@param {type:\"string\"}\n",
        "\n",
        "tokens_to_generate = 8192 #@param {type:\"slider\", min:0, max:8192, step:16}\n",
        "time_coefficient   = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "top_k_coefficient  = 5 #@param {type:\"slider\", min:2, max:50, step:1}\n",
        "%cd /content/\n",
        "\n",
        "if model is not None:\n",
        "    generated = generate_score_from_rhythm(model, prompt, rhythm2int, int2score)\n",
        "with open(\"../content/output.txt\", \"w\") as outfile:\n",
        "    outfile.write(generated)\n",
        "import tqdm\n",
        "import os\n",
        "import dill as pickle\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import floor\n",
        "from pyknon.genmidi import Midi\n",
        "from pyknon.music import NoteSeq, Note\n",
        "import music21\n",
        "import random\n",
        "import os, argparse\n",
        "\n",
        "# default settings: sample_freq=12, note_range=62\n",
        "\n",
        "def decoder(filename):\n",
        "\n",
        "    filedir = '/content/'\n",
        "\n",
        "    notetxt = filedir + filename\n",
        "\n",
        "    with open(notetxt, 'r') as file:\n",
        "        notestring=file.read()\n",
        "\n",
        "    score_note = notestring.split(\" \")\n",
        "\n",
        "    # define some parameters (from encoding script)\n",
        "    sample_freq=sample_freq_variable\n",
        "    note_range=note_range_variable\n",
        "    note_offset=note_offset_variable\n",
        "    chamber=chamber_option\n",
        "    numInstruments=number_of_instruments\n",
        "\n",
        "    # define variables and lists needed for chord decoding\n",
        "    speed=time_coefficient/sample_freq\n",
        "    piano_notes=[]\n",
        "    violin_notes=[]\n",
        "    time_offset=0\n",
        "\n",
        "    # start decoding here\n",
        "    score = score_note\n",
        "\n",
        "    i=0\n",
        "\n",
        "    # for outlier cases, not seen in sonat-1.txt\n",
        "    # not exactly sure what scores would have \"p_octave_\" or \"eoc\" (end of chord?)\n",
        "    # it seems to insert new notes to the score whenever these conditions are met\n",
        "    while i<len(score):\n",
        "        if score[i][:9]==\"p_octave_\":\n",
        "            add_wait=\"\"\n",
        "            if score[i][-3:]==\"eoc\":\n",
        "                add_wait=\"eoc\"\n",
        "                score[i]=score[i][:-3]\n",
        "            this_note=score[i][9:]\n",
        "            score[i]=\"p\"+this_note\n",
        "            score.insert(i+1, \"p\"+str(int(this_note)+12)+add_wait)\n",
        "            i+=1\n",
        "        i+=1\n",
        "\n",
        "\n",
        "    # loop through every event in the score\n",
        "    for i in tqdm.tqdm(range(len(score))):\n",
        "\n",
        "        # if the event is a blank, space, \"eos\" or unknown, skip and go to next event\n",
        "        if score[i] in [\"\", \" \", \"<eos>\", \"<unk>\"]:\n",
        "            continue\n",
        "\n",
        "        # if the event starts with 'end' indicating an end of note\n",
        "        elif score[i][:3]==\"end\":\n",
        "\n",
        "            # if the event additionally ends with eoc, increare the time offset by 1\n",
        "            if score[i][-3:]==\"eoc\":\n",
        "                time_offset+=1\n",
        "            continue\n",
        "\n",
        "        # if the event is wait, increase the timestamp by the number after the \"wait\"\n",
        "        elif score[i][:4]==\"wait\":\n",
        "            time_offset+=int(score[i][4:])\n",
        "            continue\n",
        "\n",
        "        # in this block, we are looking for notes\n",
        "        else:\n",
        "            # Look ahead to see if an end<noteid> was generated\n",
        "            # soon after.\n",
        "            duration=1\n",
        "            has_end=False\n",
        "            note_string_len = len(score[i])\n",
        "            for j in range(1,200):\n",
        "                if i+j==len(score):\n",
        "                    break\n",
        "                if score[i+j][:4]==\"wait\":\n",
        "                    duration+=int(score[i+j][4:])\n",
        "                if score[i+j][:3+note_string_len]==\"end\"+score[i] or score[i+j][:note_string_len]==score[i]:\n",
        "                    has_end=True\n",
        "                    break\n",
        "                if score[i+j][-3:]==\"eoc\":\n",
        "                    duration+=1\n",
        "\n",
        "            if not has_end:\n",
        "                duration=12\n",
        "\n",
        "            add_wait = 0\n",
        "            if score[i][-3:]==\"eoc\":\n",
        "                score[i]=score[i][:-3]\n",
        "                add_wait = 1\n",
        "\n",
        "            try:\n",
        "                new_note=music21.note.Note(int(score[i][1:])+note_offset)\n",
        "                new_note.duration = music21.duration.Duration(duration*speed)\n",
        "                new_note.offset=time_offset*speed\n",
        "                if score[i][0]==\"v\":\n",
        "                    violin_notes.append(new_note)\n",
        "                else:\n",
        "                    piano_notes.append(new_note)\n",
        "            except:\n",
        "                print(\"Unknown note: \" + score[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            time_offset+=add_wait\n",
        "\n",
        "    # list of all notes for each instrument should be ready at this stage\n",
        "\n",
        "    # creating music21 instrument objects\n",
        "\n",
        "    piano=music21.instrument.fromString(\"Accordion\")\n",
        "    violin=music21.instrument.fromString(\"Violin\")\n",
        "\n",
        "    # insert instrument object to start (0 index) of notes list\n",
        "\n",
        "    piano_notes.insert(0, piano)\n",
        "    violin_notes.insert(0, violin)\n",
        "    # create music21 stream object for individual instruments\n",
        "\n",
        "    piano_stream=music21.stream.Stream(piano_notes)\n",
        "    violin_stream=music21.stream.Stream(violin_notes)\n",
        "    # merge both stream objects into a single stream of 2 instruments\n",
        "    note_stream = music21.stream.Stream([piano_stream, violin_stream])\n",
        "\n",
        "\n",
        "    note_stream.write('midi', fp=\"/content/\"+filename[:-4]+\".mid\")\n",
        "    print(\"Done! Decoded midi file saved to 'content/'\")\n",
        "\n",
        "\n",
        "decoder('output.txt')\n",
        "from google.colab import files\n",
        "files.download('/content/output.mid')"
      ],
      "metadata": {
        "id": "PATCNGHeMURi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qR53cL5sU5x"
      },
      "source": [
        "#@title (OPTION 1) Save the trained Model from memory\n",
        "torch.save(net, '/content/trained_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4WwBmm-cRVX"
      },
      "source": [
        "#@title (OPTION 2) Download Super Chamber Piano Pre-Trained Chamber Model\n",
        "%cd /content/\n",
        "!wget 'https://github.com/asigalov61/SuperPiano/raw/master/trained_model.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAWqshRRod62"
      },
      "source": [
        "#@title (OPTION 2) Load existing/pre-trained Model checkpoint\n",
        "model = torch.load('/content/trained_model.h5', map_location='cpu', weights_only=False)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abc41SL00kkf"
      },
      "source": [
        "# Generate, plot, and listen to the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkgZfG_u4cPt"
      },
      "source": [
        "#@title Generate TXT and MIDI file\n",
        "seed_prompt = \"p24\" #@param {type:\"string\"}\n",
        "tokens_to_generate = 8192 #@param {type:\"slider\", min:0, max:8192, step:16}\n",
        "time_coefficient = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "top_k_coefficient = 5 #@param {type:\"slider\", min:2, max:50, step:1}\n",
        "%cd /content/\n",
        "with open(\"../content/output.txt\", \"w\") as outfile:\n",
        "    outfile.write(' '.join([int2word[int(int_)] for int_ in model.predict(seed_seq=seed_prompt, pred_len=tokens_to_generate, top_k=top_k_coefficient)]))\n",
        "import tqdm\n",
        "import os\n",
        "import dill as pickle\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import floor\n",
        "from pyknon.genmidi import Midi\n",
        "from pyknon.music import NoteSeq, Note\n",
        "import music21\n",
        "import random\n",
        "import os, argparse\n",
        "\n",
        "# default settings: sample_freq=12, note_range=62\n",
        "\n",
        "def decoder(filename):\n",
        "\n",
        "    filedir = '/content/'\n",
        "\n",
        "    notetxt = filedir + filename\n",
        "\n",
        "    with open(notetxt, 'r') as file:\n",
        "        notestring=file.read()\n",
        "\n",
        "    score_note = notestring.split(\" \")\n",
        "\n",
        "    # define some parameters (from encoding script)\n",
        "    sample_freq=sample_freq_variable\n",
        "    note_range=note_range_variable\n",
        "    note_offset=note_offset_variable\n",
        "    chamber=chamber_option\n",
        "    numInstruments=number_of_instruments\n",
        "\n",
        "    # define variables and lists needed for chord decoding\n",
        "    speed=time_coefficient/sample_freq\n",
        "    piano_notes=[]\n",
        "    violin_notes=[]\n",
        "    time_offset=0\n",
        "\n",
        "    # start decoding here\n",
        "    score = score_note\n",
        "\n",
        "    i=0\n",
        "\n",
        "    # for outlier cases, not seen in sonat-1.txt\n",
        "    # not exactly sure what scores would have \"p_octave_\" or \"eoc\" (end of chord?)\n",
        "    # it seems to insert new notes to the score whenever these conditions are met\n",
        "    while i<len(score):\n",
        "        if score[i][:9]==\"p_octave_\":\n",
        "            add_wait=\"\"\n",
        "            if score[i][-3:]==\"eoc\":\n",
        "                add_wait=\"eoc\"\n",
        "                score[i]=score[i][:-3]\n",
        "            this_note=score[i][9:]\n",
        "            score[i]=\"p\"+this_note\n",
        "            score.insert(i+1, \"p\"+str(int(this_note)+12)+add_wait)\n",
        "            i+=1\n",
        "        i+=1\n",
        "\n",
        "\n",
        "    # loop through every event in the score\n",
        "    for i in tqdm.tqdm(range(len(score))):\n",
        "\n",
        "        # if the event is a blank, space, \"eos\" or unknown, skip and go to next event\n",
        "        if score[i] in [\"\", \" \", \"<eos>\", \"<unk>\"]:\n",
        "            continue\n",
        "\n",
        "        # if the event starts with 'end' indicating an end of note\n",
        "        elif score[i][:3]==\"end\":\n",
        "\n",
        "            # if the event additionally ends with eoc, increare the time offset by 1\n",
        "            if score[i][-3:]==\"eoc\":\n",
        "                time_offset+=1\n",
        "            continue\n",
        "\n",
        "        # if the event is wait, increase the timestamp by the number after the \"wait\"\n",
        "        elif score[i][:4]==\"wait\":\n",
        "            time_offset+=int(score[i][4:])\n",
        "            continue\n",
        "\n",
        "        # in this block, we are looking for notes\n",
        "        else:\n",
        "            # Look ahead to see if an end<noteid> was generated\n",
        "            # soon after.\n",
        "            duration=1\n",
        "            has_end=False\n",
        "            note_string_len = len(score[i])\n",
        "            for j in range(1,200):\n",
        "                if i+j==len(score):\n",
        "                    break\n",
        "                if score[i+j][:4]==\"wait\":\n",
        "                    duration+=int(score[i+j][4:])\n",
        "                if score[i+j][:3+note_string_len]==\"end\"+score[i] or score[i+j][:note_string_len]==score[i]:\n",
        "                    has_end=True\n",
        "                    break\n",
        "                if score[i+j][-3:]==\"eoc\":\n",
        "                    duration+=1\n",
        "\n",
        "            if not has_end:\n",
        "                duration=12\n",
        "\n",
        "            add_wait = 0\n",
        "            if score[i][-3:]==\"eoc\":\n",
        "                score[i]=score[i][:-3]\n",
        "                add_wait = 1\n",
        "\n",
        "            try:\n",
        "                new_note=music21.note.Note(int(score[i][1:])+note_offset)\n",
        "                new_note.duration = music21.duration.Duration(duration*speed)\n",
        "                new_note.offset=time_offset*speed\n",
        "                if score[i][0]==\"v\":\n",
        "                    violin_notes.append(new_note)\n",
        "                else:\n",
        "                    piano_notes.append(new_note)\n",
        "            except:\n",
        "                print(\"Unknown note: \" + score[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            time_offset+=add_wait\n",
        "\n",
        "    # list of all notes for each instrument should be ready at this stage\n",
        "\n",
        "    # creating music21 instrument objects\n",
        "\n",
        "    piano=music21.instrument.fromString(\"Accordion\")\n",
        "    violin=music21.instrument.fromString(\"Violin\")\n",
        "\n",
        "    # insert instrument object to start (0 index) of notes list\n",
        "\n",
        "    piano_notes.insert(0, piano)\n",
        "    violin_notes.insert(0, violin)\n",
        "    # create music21 stream object for individual instruments\n",
        "\n",
        "    piano_stream=music21.stream.Stream(piano_notes)\n",
        "    violin_stream=music21.stream.Stream(violin_notes)\n",
        "    # merge both stream objects into a single stream of 2 instruments\n",
        "    note_stream = music21.stream.Stream([piano_stream, violin_stream])\n",
        "\n",
        "\n",
        "    note_stream.write('midi', fp=\"/content/\"+filename[:-4]+\".mid\")\n",
        "    print(\"Done! Decoded midi file saved to 'content/'\")\n",
        "\n",
        "\n",
        "decoder('output.txt')\n",
        "from google.colab import files\n",
        "files.download('/content/output.mid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP-a6xhKp6NQ"
      },
      "source": [
        "#@title Plot, Graph, and Listen to the Output :)\n",
        "graphs_length_inches = 18 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "notes_graph_height = 6 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "highest_displayed_pitch = 92 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "lowest_displayed_pitch = 24 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "midi_data = pretty_midi.PrettyMIDI('/content/output.mid')\n",
        "\n",
        "def plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n",
        "    # Use librosa's specshow function for displaying the piano roll\n",
        "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
        "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
        "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
        "\n",
        "\n",
        "\n",
        "roll = np.zeros([int(graphs_length_inches), 128])\n",
        "# Plot the output\n",
        "\n",
        "track = Multitrack('/content/output.mid')\n",
        "print(track)\n",
        "plt.figure(figsize=[graphs_length_inches, notes_graph_height])\n",
        "fig, ax = track.plot()\n",
        "fig.set_size_inches(graphs_length_inches, notes_graph_height)\n",
        "plt.figure(figsize=[graphs_length_inches, notes_graph_height])\n",
        "ax2 = plot_piano_roll(midi_data, int(lowest_displayed_pitch), int(highest_displayed_pitch))\n",
        "plt.show(block=False)\n",
        "\n",
        "\n",
        "FluidSynth(\"/content/font.sf2\", 16000).midi_to_audio('/content/output.mid', '/content/output.wav')\n",
        "Audio('/content/output.wav', rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ajJYg7RwqIk"
      },
      "source": [
        "#@title Reward yourself by making a nice Arc diagram from the generated output/MIDI file\n",
        "%cd '/content/arc-diagrams'\n",
        "\n",
        "midi_file = '/content/output.mid'\n",
        "plot_title = \"Super Chamber Piano Output Arc Diagram\"\n",
        "\n",
        "# midi_file = 'midis/fuer_elise.mid'\n",
        "# plot_title = \"Für Elise (Beethoven)\"\n",
        "\n",
        "\n",
        "def stringify_notes(midi_file, track_number ):\n",
        "\n",
        "    mid = MidiFile(midi_file)\n",
        "    track_notes = {}\n",
        "    for i, track in enumerate(mid.tracks):\n",
        "        track_notes[i] = ''\n",
        "        for msg in track:\n",
        "            if( msg.type == 'note_on'):\n",
        "                track_notes[i] += str(msg.note) +'n'\n",
        "            if( msg.type == 'note_off'):\n",
        "                track_notes[i] += str(msg.note) +'f'\n",
        "    return track_notes[track_number]\n",
        "try:\n",
        "  plot_arc_diagram(stringify_notes(midi_file, 0), plot_title)\n",
        "  from google.colab import files\n",
        "  files.download('/content/arc-diagrams/output.png')\n",
        "except:\n",
        "  print('Could not plot the diagram. Try again/another composition.')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhNSpt7AzYsV"
      },
      "source": [
        "# Save what you want to Google Drive (standard GD connect code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m-LweGaY3a1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "id": "T-D9UXLoBfIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (int2word)"
      ],
      "metadata": {
        "id": "umUQYsdpBgtj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}